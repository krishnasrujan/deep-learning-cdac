{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S16 RNN LSTM Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Location \n",
    "http://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption\n",
    "\n",
    "**Attribute Information:**\n",
    "\n",
    "1. date: Date in format dd/mm/yyyy \n",
    "2. time: time in format hh:mm:ss \n",
    "3. global_active_power: household global minute-averaged active power (in kilowatt) \n",
    "4. global_reactive_power: household global minute-averaged reactive power (in kilowatt) \n",
    "5. voltage: minute-averaged voltage (in volt) \n",
    "6. global_intensity: household global minute-averaged current intensity (in ampere) \n",
    "7. sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). \n",
    "8. sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light. \n",
    "9. sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment:\n",
    "This is a supervised learning problem. Formulated it to predict the `Global_active_power` at the current time `t` given the `Global_active_power` measurement and all other other features at the prior time step.\n",
    "\n",
    "You can chose to use `Global_active_power` alone to compare the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Clean up  missing values and 'nan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Data Preparation and feature engineering\n",
    "\n",
    "- Think how can you prepare features out of given records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "1. LSTM with 100 neurons in the first visible layer (**Q:** would you like ot change it and why?)\n",
    "2. dropout 20%\n",
    "4. 1 neuron in the output layer for predicting Global_active_power. \n",
    "5. The input shape will be 1 time step with 7 features.\n",
    "6. Use the Mean Absolute Error (MAE) loss function and the efficient Adam gradient descent.\n",
    "7. The model will be fit for suitable training epochs with a suitable batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "model.add(tf.keras.layers.LSTM(100,\n",
    "                               activation='relu',\n",
    "                               kernel_initializer=initializer,\n",
    "                               input_shape=(X_train.shape[1],\n",
    "                                            X_train.shape[2])))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> tf.keras.layers.LSTM(\n",
    "    units, activation='tanh', recurrent_activation='sigmoid', use_bias=True,\n",
    "    kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
    "    bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None,\n",
    "    recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,\n",
    "    dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False,\n",
    "    return_state=False, go_backwards=False, stateful=False, time_major=False,\n",
    "    unroll=False, **kwargs\n",
    ")\n",
    "\n",
    "## Arguments: \n",
    "Arguments|Description\n",
    ":--|:--\n",
    "units|Positive integer, dimensionality of the output space.\n",
    "activation|Activation function to use. Default: hyperbolic tangent (tanh). If you pass None, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "recurrent_activation|Activation function to use for the recurrent step. Default: sigmoid (sigmoid). If you pass None, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "use_bias|Boolean (default True), whether the layer uses a bias vector.\n",
    "kernel_initializer|Initializer for the kernel weights matrix, used for the linear transformation of the inputs. Default: glorot_uniform.\n",
    "recurrent_initializer|Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. Default: orthogonal.\n",
    "bias_initializer|Initializer for the bias vector. Default: zeros.\n",
    "unit_forget_bias|Boolean (default True). If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\". This is recommended in Jozefowicz et al..\n",
    "kernel_regularizer|Regularizer function applied to the kernel weights matrix. Default: None.\n",
    "recurrent_regularizer|Regularizer function applied to the recurrent_kernel weights matrix. Default: None.\n",
    "bias_regularizer|Regularizer function applied to the bias vector. Default: None.\n",
    "activity_regularizer|Regularizer function applied to the output of the layer (its \"activation\"). Default: None.\n",
    "kernel_constraint|Constraint function applied to the kernel weights matrix. Default: None.\n",
    "recurrent_constraint|Constraint function applied to the recurrent_kernel weights matrix. Default: None.\n",
    "bias_constraint|Constraint function applied to the bias vector. Default: None.\n",
    "dropout|Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. Default: 0.\n",
    "recurrent_dropout|Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. Default: 0.\n",
    "implementation|Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. Default: 2.\n",
    "return_sequences|Boolean. Whether to return the last output. in the output sequence, or the full sequence. Default: False.\n",
    "return_state|Boolean. Whether to return the last state in addition to the output. Default: False.\n",
    "go_backwards|Boolean (default False). If True, process the input sequence backwards and return the reversed sequence.\n",
    "stateful|Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n",
    "time_major|The shape format of the inputs and outputs tensors. If True, the inputs and outputs will be in shape [timesteps, batch, feature], whereas in the False case, it will be [batch, timesteps, feature]. Using time_major = True is a bit more efficient because it avoids transposes at the beginning and end of the RNN calculation. However, most TensorFlow data is batch-major, so by default this function accepts input and emits output in batch-major form.\n",
    "unroll|Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets scale the X_test too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scale pred\n",
    "\n",
    "# invert scale for actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE\n",
    "\n",
    "# calculate R2 Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
