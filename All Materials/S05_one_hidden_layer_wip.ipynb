{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 5: NN with one hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">\n",
    "By Pramod Sharma : pramod.sharma@prasami.com\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import some libraries\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic parameters\n",
    "\n",
    "inpDir = '../input' # location where input data is stored\n",
    "outDir = '../output' # location to store outputs\n",
    "RANDOM_STATE = 24 # for initialization ----- REMEMBER: to remove at the time of promotion to production\n",
    "EPOCHS = 20000 # number of cycles to run\n",
    "\n",
    "# Set parameters for decoration of plots\n",
    "params = {'legend.fontsize' : 'large',\n",
    "          'figure.figsize'  : (12,9),\n",
    "          'axes.labelsize'  : 'x-large',\n",
    "          'axes.titlesize'  :'x-large',\n",
    "          'xtick.labelsize' :'large',\n",
    "          'ytick.labelsize' :'large',\n",
    "         }\n",
    "\n",
    "plt.rcParams.update(params) # update rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data Set\n",
    "<p style=\"font-family: Arial; font-size:1.1em;color:blue;\">\n",
    "Use Sklearn's dataset generator <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html\">make_moon</a>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=1280, shuffle=True, noise=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.1em;color:brown;\">\n",
    "<strong>Note</strong>: All two dimensional matrix are represented by Caps and all arrays (vectors) are represented by small case.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Plot the data\n",
    "plt.scatter(X[:,0], X[:,1], s=30, c=y, cmap=plt.cm.Spectral)\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_decision_boundary(pred_func, X, y):\n",
    "    '''\n",
    "        Attrib:\n",
    "           pred_func : function based on predict method of the classifier\n",
    "           X : feature matrix\n",
    "           y : targets\n",
    "       Return:\n",
    "           None\n",
    "    '''\n",
    "    \n",
    "    # Set min and max values and give it some padding\n",
    "    xMin, xMax = X[:, 0].min() - .05, X[:, 0].max() + .05\n",
    "    yMin, yMax = X[:, 1].min() - .05, X[:, 1].max() + .05\n",
    "    \n",
    "    # grid size for mesh grid\n",
    "    h = 0.01\n",
    "    \n",
    "    # Generate a grid of points with distance 'h' between them\n",
    "    xx, yy = np.meshgrid(np.arange(xMin, xMax, h), np.arange(yMin, yMax, h))\n",
    "    \n",
    "    # Predict the function value for the whole grid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Make its shape same as that of xx \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Now we have Z value corresponding to each of the combination of xx and yy\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    \n",
    "    # plot the points as well\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Split the data in training and test sets to measure performance of the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE )\n",
    "\n",
    "print (X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\"> \n",
    "    Let's start with simple network. Our data has <strong>two</strong> features. Hence size of input layer will also be two. The output is binary, we can code it as single column as well as double column output. The hidden layer could be of <strong>any size</strong>. One need to execute a handful of iterations to arrive at right size of hidden layer. For purpose of this demo, size of hidden layer is taken as shown below.\n",
    "</p>\n",
    "<img src='../Presentations/images/S5/nn_S5_f1.jpg' style='width: 100'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "We need to minimise the error by adjusting ($W_1, b_1$). We call the function that measures our error the *loss function*. A common choice with the softmax output is the cross-entropy loss. The loss for our prediction $\\hat{y}$ with respect to the true labels $y$ is given by:\n",
    "</p>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\hat{y},y) =  -y.log\\hat{y} - (1-y) . log(1-\\hat{y})\n",
    "\\end{aligned}\n",
    "$$\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "For all samples:\n",
    "</p>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J(\\hat{y}, y) =  -\\frac{1}{m}\\sum_{i \\in m}y_i.log\\hat{y_i} - (1-y_i) . log(1-\\hat{y_i})\n",
    "\\end{aligned}\n",
    "$$\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "    We can use gradient descent to find its minimum. for purpose of this demonstration lets use it in its simplest form - <strong>batch gradient descent with fixed learning rate</strong>.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "    We will be using $tanh$ function for layer 1 (hidden layer) as it is most common which fits in majority of cases and its derivative can simply be represented as 1 -$\\tanh^2(z_1)$. Since our output is binary, it makes sense to use $\\mathrm{Softmax} / \\mathrm{Sigmoid}$.\n",
    "</p>\n",
    "<img src='../Presentations/images/S5/nn_S5_f2.jpg' style='width: 100'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "$\n",
    "\\begin{aligned}\n",
    "z_1^{[1]} & = x . W_1^{[1]} + b_1^{[1]}\\\\\n",
    "a_1^{[1]} & = \\tanh(z_1^{[1]}) \\\\\n",
    "\\\\\n",
    "z_2^{[1]} & = x . W_2^{[1]} + b_2^{[1]} \\\\\n",
    "a_2^{[1]} & = \\tanh(z_2^{[1]}) \\\\\n",
    "\\\\\n",
    "z_3^{[1]} & = x . W_3^{[1]} + b_3^{[1]} \\\\\n",
    "a_3^{[1]} & = \\tanh(z_3^{[1]}) \\\\\n",
    "\\\\\n",
    "z_4^{[1]} & = x . W_4^{[1]} + b_4^{[1]} \\\\\n",
    "a_4^{[1]} & = \\tanh(z_4^{[1]}) \\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "<hr>\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "    If we convert above to matrix version, we can say.</p>\n",
    "$\n",
    "\\begin{aligned}\n",
    "Z^{[1]} & = X . W^{[1]} + b^{[1]} \\\\\n",
    "a^{[1]} & = \\tanh(z^{[1]}) \\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "    Similarly for second layer.\n",
    "</p>\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "z^{[2]} & = a^{[1]}. W^{[2]} + b^{[2]} \\\\\n",
    "a^{[2]} & = \\hat{y} = \\mathrm{softmax}(z^{[2]})\\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "    Where:\n",
    "</p>\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathrm{softmax}(z_i) & =  \\frac{e^{z_i}}{\\sum_{i=1}^n {e^{z_i}}}\\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "<p> Above derivation covers all layers and one training row.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate the total loss on the dataset\n",
    "\n",
    "def calculate_loss(model, X, y):\n",
    "    \n",
    "    # extract weights and losses from the model\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # Forward propagation to calculate our predictions\n",
    "    z1 = X.dot(W1) + b1\n",
    "    \n",
    "    # Tanh activation\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # softmax activation\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculating the loss\n",
    "    # Cross entropy = ground truth x log (predicted)\n",
    "    # probability of y being correct is 1. hence it will be a vector of [1,1,...,1,1]\n",
    "    \n",
    "    correct_logprobs = -np.log(probs[range(num_examples), y]) \n",
    "    data_loss = np.sum(correct_logprobs)\n",
    "\n",
    "    \n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Function\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "For predictions, we will simply be using the forward propagation. No need to iterate or calculate the back propagation for supervised learning.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "\n",
    "def predict(model, x):\n",
    "    '''\n",
    "     Args:\n",
    "         model\n",
    "         x: input features\n",
    "    '''\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # Forward propagation\n",
    "    z1 = x.dot(W1) + b1\n",
    "    \n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # use softmax\n",
    "    exp_scores = np.exp(z2)\n",
    "    \n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    return np.argmax(probs, axis=1) # pick with one with highest probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../Presentations/images/S5/nn_S5_f3.jpg' style='width: 800px;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "As an input, gradient descent needs the gradients (vector of derivatives) of the loss function with respect to our parameters: $\\frac{\\partial{L}}{\\partial{W_1}}(= \\partial{W^{[1]}})$, $\\frac{\\partial{L}}{\\partial{b_1}}(= \\partial{b^{[1]}})$, $\\frac{\\partial{L}}{\\partial{W_2}}(= \\partial{W^{[2]}})$, $\\frac{\\partial{L}}{\\partial{b_2}}(= \\partial{b^{[2]}})$. To calculate these gradients we use the <strong>backpropagation algorithm,</strong>. Following calculations are Matrix form of single-row-equations shown above.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\partial{Z^{[2]}}  & = A^{[2]} - Y \\\\\n",
    "\\partial{W^{[2]}}  & = \\frac{1}{m} A^{[1]T}\\circ \\partial{Z^{[2]}} \\\\\n",
    "\\partial{b^{[2]}}  & = \\frac{1}{m} \\mathrm{np.sum}(\\partial{Z^{[2]}}, axis = 1, keepdims = True) \\\\\n",
    "\\\\\n",
    "\\partial{Z^{[1]}}  & = \\partial{Z^{[2]}}\\circ  W^{[2]T} * ( 1-A^{[1]}**2)\\\\\n",
    "\\partial{W^{[1]}}  & = \\frac{1}{m} X^{T}\\circ \\partial{Z^{[1]}} \\\\\n",
    "\\partial{b^{[1]}}  & = \\frac{1}{m} \\mathrm{np.sum}(\\partial{Z^{[1]}}, axis = 1, keepdims = True) \\\\\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the Model\n",
    "\n",
    "def build_model(nn_hdim, X, y, EPOCHS=20000,  print_loss=False):\n",
    "    \n",
    "    '''\n",
    "        nn_hdim : Number of nodes in the hidden layer\n",
    "        X : Features to train on\n",
    "        y : Targets to train on\n",
    "        EPOCHS : Number of passes through the training data for gradient descent\n",
    "        print_loss : If True, print the loss every nnn iterations\n",
    "    '''\n",
    "    # set Random Seed\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    \n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "    curr_loss = 0\n",
    "    loss = []\n",
    "    epoch = []\n",
    "    \n",
    "    # Gradient descent. For each batch...\n",
    "    for i in range(0, EPOCHS):\n",
    "        \n",
    "        ##########################\n",
    "        #   Forward propagation  #\n",
    "        ##########################\n",
    "        \n",
    "        # Layer 1\n",
    "        z1 = X.dot(W1) + b1\n",
    "        a1 = np.tanh(z1)  # tanh activation function for layer 1\n",
    "        \n",
    "        # Layer 2\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        exp_scores = np.exp(z2) # softmax for final layer as it is binary classification\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        #######################\n",
    "        #   Back propagation  #\n",
    "        #######################\n",
    "        # Layer 2\n",
    "        dz2 = probs\n",
    "        dz2[range(num_examples), y] -= 1 # dL/db = dL/dz = (a-y). \n",
    "        #                                As Y is single dimension subtract one from its class\n",
    "        \n",
    "        dW2 = (a1.T).dot(dz2)/num_examples\n",
    "        assert(W2.shape == dW2.shape), 'Shape of W2 {} and dW2 {} do not match'.format(W2.shape, dW2.shape)\n",
    "        \n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / num_examples # db2 is vertical sum\n",
    "        assert(b2.shape == db2.shape), 'Shape of b2 {} and db2 {} do not match'.format(b2.shape, db2.shape)\n",
    "            \n",
    "        dz1 = dz2.dot(W2.T) * (1 - np.power(a1, 2))  #derivative of tanh is (1âˆ’tanh(x)**2)\n",
    "        assert(z1.shape == dz1.shape), 'Shape of z1 {} and dz1 {} do not match'.format(W2.shape, dW2.shape)\n",
    "        \n",
    "        dW1 = np.dot(X.T, dz1)/num_examples\n",
    "        assert(W1.shape == dW1.shape), 'Shape of W1 {} and dW1 {} do not match'.format(W1.shape, dW1.shape)\n",
    "        db1 = np.sum(dz1, axis=0)/num_examples # Note changing dim of db1\n",
    "        \n",
    "        \n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        \n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        \n",
    "        if i % 100:\n",
    "            curr_loss = calculate_loss(model, X, y)\n",
    "            loss.append(curr_loss)\n",
    "            epoch.append(i)\n",
    "        \n",
    "        # Print the loss.\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" %(i, curr_loss))\n",
    "            \n",
    "    curr_loss = calculate_loss(model, X, y)\n",
    "    loss.append(curr_loss)\n",
    "    epoch.append(i)\n",
    "    print(\"Loss after iteration %i: %f\" %(i, curr_loss))\n",
    "    \n",
    "    loss_hist['epoch'] = epoch\n",
    "    loss_hist['loss'] = loss\n",
    "    \n",
    "    return model, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(X_train) # training set size\n",
    "nn_input_dim = 2 # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality\n",
    "\n",
    "# lists to facilitate plotting \n",
    "loss_hist = {}\n",
    "\n",
    "# Gradient descent parameters\n",
    "# Try following values of epsilon to see its effect on the graph\n",
    "#[ 0.0001, 0.001, 0.1, 1]\n",
    "epsilon = 0.1 # learning rate for gradient descent Try \n",
    "reg_lambda = 0.0 # regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model with a 4-dimensional hidden layer\n",
    "model, probs = build_model(4, X_train, y_train, EPOCHS = EPOCHS, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(loss_hist)\n",
    "\n",
    "fn_plot_decision_boundary(lambda x: predict(model, x), X_train, y_train) # plot decision boundary for this plot\n",
    "\n",
    "plt.title(\"Decision Boundary\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2 , figsize = (15,6))\n",
    "\n",
    "l_range = 100\n",
    "\n",
    "ax = axes[0]\n",
    "\n",
    "loss_df.plot(x = 'epoch', y = 'loss', ax = ax)\n",
    "loss = loss_df['loss'].values\n",
    "\n",
    "# little beautification\n",
    "txtstr = \"Errors: \\n  Start : {:7.4f}\\n   End : {:7.4f}\".format(loss[0],loss[-1]) #text to plot\n",
    "# properties  matplotlib.patch.Patch \n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "\n",
    "ax.text(0.6, 0.95, txtstr, transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_title('Overall')\n",
    "ax.grid();\n",
    "\n",
    "ax = axes[1]\n",
    "\n",
    "loss_df[-l_range:].plot(x = 'epoch', y = 'loss', ax = ax)\n",
    "\n",
    "# little beautification\n",
    "txtstr = \"Errors: \\n  Start : {:7.4f}\\n   End : {:7.4f}\".format(loss[-l_range],loss[-1]) #text to plot\n",
    "# properties  matplotlib.patch.Patch \n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "\n",
    "ax.text(0.6, 0.95, txtstr, transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_title('Last {} records'.format(l_range))\n",
    "ax.grid();\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_predicitions(pred_func, X):\n",
    "    y_pred = pred_func(X)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fn_make_predicitions(lambda x: predict(model, x), X_train)\n",
    "print('Accuracy score on Train Data :', accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fn_make_predicitions(lambda x: predict(model, x), X_test)\n",
    "\n",
    "print('Accuracy score on Test Data :', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''plt.figure(figsize=(10, 15)) # Set plot size\n",
    "\n",
    "hidden_layer_dimensions = [1, 2, 3, 4, 5, 20] # number of layers to iterate\n",
    "\n",
    "for i, nn_hdim in enumerate(hidden_layer_dimensions):\n",
    "\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    \n",
    "    plt.title('Hidden Layer size %d' % nn_hdim)\n",
    "    \n",
    "    model, probs = build_model(nn_hdim, X_train, y_train ,EPOCHS = EPOCHS,) # calling build model\n",
    "    \n",
    "    fn_plot_decision_boundary(lambda x: predict(model, x), X_train, y_train )\n",
    "        \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure() # Set plot size\n",
    "\n",
    "nn_hdim = 50 # number of layers to iterate\n",
    "\n",
    "plt.subplot()\n",
    "\n",
    "plt.title('Hidden Layer size %d' % nn_hdim)\n",
    "\n",
    "model, probs = build_model(nn_hdim, X_train, y_train , EPOCHS = EPOCHS, print_loss = True) # calling build model\n",
    "\n",
    "fn_plot_decision_boundary(lambda x: predict(model, x), X_train, y_train )\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(4, activation='tanh'),\n",
    "  tf.keras.layers.Dense(2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent (with momentum) optimizer.\n",
    "\n",
    ">tf.keras.optimizers.SGD(\n",
    ">   learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD', **kwargs\n",
    ">)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train , epochs = 5000, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = probability_model(X_test).numpy().argmax(axis = 1)\n",
    "print('Accuracy score on Test Data :', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
