{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6 : Multiple Hidden Layers, Multi-class, with model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "By Pramod Sharma : pramod.sharma@prasami.com\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import some libraries\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic parameters\n",
    "\n",
    "inpDir = '../input' # location where input data is stored\n",
    "outDir = '../output' # location to store outputs\n",
    "modelfile = 'model.pkl'\n",
    "RANDOM_STATE = 24 # for initialization ----- REMEMBER: to remove at the time of promotion to production\n",
    "EPOCHS = 20000 # number of cycles to run\n",
    "\n",
    "# Set parameters for decoration of plots\n",
    "params = {'legend.fontsize' : 'large',\n",
    "          'figure.figsize'  : (9,6),\n",
    "          'axes.labelsize'  : 'x-large',\n",
    "          'axes.titlesize'  :'x-large',\n",
    "          'xtick.labelsize' :'large',\n",
    "          'ytick.labelsize' :'large',\n",
    "         }\n",
    "\n",
    "plt.rcParams.update(params) # update rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Set\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "Use Fashion MNIST dataset.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = os.path.join(inpDir, 'fashion_mnist', 'fashion-mnist_train.csv')\n",
    "test_filename = os.path.join(inpDir, 'fashion_mnist', 'fashion-mnist_test.csv')\n",
    "\n",
    "train_df = pd.read_csv(train_filename, header = 0)\n",
    "test_df = pd.read_csv(test_filename, header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = train_df.sample(n = 50) \n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))  # figure size in inches\n",
    "\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "i = 0\n",
    "for count, row in plot_df.iterrows():\n",
    "    i = i + 1\n",
    "    image = row.values[1:].reshape(28,28)\n",
    "    ax = fig.add_subplot(5, 10, (i), xticks=[], yticks=[])\n",
    "    ax.imshow(image, cmap=plt.cm.binary, interpolation='nearest')\n",
    "    ax.text(6, 7, str(row[0]), color='red', fontsize=14)   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = OneHotEncoder(sparse=False)\n",
    "y_train = train_df[[\"label\"]]\n",
    "X_train = train_df.drop('label', axis = 1).values\n",
    "y_train = one_hot.fit_transform(y_train)\n",
    "\n",
    "y_test = test_df[[\"label\"]]\n",
    "X_test = test_df.drop('label', axis = 1).values\n",
    "y_test = one_hot.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_decision_boundary(pred_func, X, y):\n",
    "    '''\n",
    "        Attrib:\n",
    "           pred_func : function based on predict method of the classifier\n",
    "           X : feature matrix\n",
    "           y : targets\n",
    "       Return:\n",
    "           None           \n",
    "    '''\n",
    "    \n",
    "    # Set min and max values and give it some padding\n",
    "    xMin, xMax = X[:, 0].min() - .05, X[:, 0].max() + .05\n",
    "    yMin, yMax = X[:, 1].min() - .05, X[:, 1].max() + .05\n",
    "    \n",
    "    # grid size for mesh grid\n",
    "    h = 0.01\n",
    "    \n",
    "    # Generate a grid of points with distance 'h' between them\n",
    "    xx, yy = np.meshgrid(np.arange(xMin, xMax, h), np.arange(yMin, yMax, h))\n",
    "    \n",
    "    # Predict the function value for the whole grid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Make its shape same as that of xx \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Now we have Z value corresponding to each of the combination of xx and yy\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    \n",
    "    # plot the points as well\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Split the data in training and test sets to measure performance of the model.\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE )\n",
    "\n",
    "#print (X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\"> \n",
    "    Moving over to multilayer network. Our data has <strong>two</strong> features. Hence size of input layer will also be two. The output is binary, we can code it as single column as well as double column output. The hidden layers will be as follows:</p>\n",
    "<table style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "    <tr>\n",
    "        <th>#</th>\n",
    "        <th>Layer Number</th>\n",
    "        <th>Nodes </th>\n",
    "        <th>Activation </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>Input Layer</td>\n",
    "        <td>2</td>\n",
    "        <td>tanh</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>Hidden Layer 1</td>\n",
    "        <td>5</td>\n",
    "        <td>tanh</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>Hidden Layer 2</td>\n",
    "        <td>5</td>\n",
    "        <td>tanh</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>4</td>\n",
    "        <td>Hidden Layer 3</td>\n",
    "        <td>4</td>\n",
    "        <td>tanh</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>5</td>\n",
    "        <td>Hidden Layer 4</td>\n",
    "        <td>3</td>\n",
    "        <td>tanh</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>6</td>\n",
    "        <td>Output Layer 4</td>\n",
    "        <td>2</td>\n",
    "        <td>softmax</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../Presentations/images/S6/nn_S6_f1.jpg' style='width: 100'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "The loss for our prediction $\\hat{y}$ with respect to the true labels $y$ is given by:\n",
    "</p>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\hat{y},y) =  -y.log\\hat{y} - (1-y) . log(1-\\hat{y})\n",
    "\\end{aligned}\n",
    "$$\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "For all samples:\n",
    "</p>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J(\\hat{y}, y) =  -\\frac{1}{m}\\sum_{i \\in m}y_i.log\\hat{y_i} - (1-y_i) . log(1-\\hat{y_i})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate the total loss on the dataset\n",
    "\n",
    "def calculate_loss(model, X, y):\n",
    "    \n",
    "    # Extract weights and losses from the model\n",
    "    W1, W2, W3, W4, W5 = model['W1'], model['W2'], model['W3'], model['W4'], model['W5']\n",
    "    b1, b2, b3, b4, b5 = model['b1'], model['b2'], model['b3'], model['b4'], model['b5']\n",
    "    \n",
    "    #***********************************\n",
    "    # Layer 1\n",
    "    Z1 = X.dot(W1) + b1 \n",
    "    A1 = np.tanh(Z1)    # tanh activation\n",
    "    assert (Z1.shape == A1.shape),\"Shape of Z1 and A1 do not match\"\n",
    "\n",
    "    # Layer 2\n",
    "    Z2 = A1.dot(W2) + b2 \n",
    "    A2 = np.tanh(Z2)    # tanh activation\n",
    "    assert (Z2.shape == A2.shape),\"Shape of Z2 and A2 do not match\"\n",
    "\n",
    "    # Layer 3\n",
    "    Z3 = A2.dot(W3) + b3 \n",
    "    A3 = np.tanh(Z3)    # tanh activation\n",
    "    assert (Z3.shape == A3.shape),\"Shape of Z3 and A3 do not match\"\n",
    "\n",
    "    # Layer 4\n",
    "    Z4 = A3.dot(W4) + b4 \n",
    "    A4 = np.tanh(Z4)    # tanh activation\n",
    "    assert (Z4.shape == A4.shape),\"Shape of Z4 and A4 do not match\"\n",
    "\n",
    "\n",
    "    Z5 = A4.dot(W5) + b5\n",
    "    exp_scores = np.exp(Z5) # softmax for final layer as it is binary classification\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    #*************************************\n",
    "        \n",
    "    # Calculating the loss\n",
    "    # Cross entropy = ground truth x log (predicted)\n",
    "    # probability of y being correct is 1. hence it will be a vector of [1,1,...,1,1]\n",
    "    #correct_logprobs = - (y.dot(np.log(probs.T)) + (1 - y).dot(np.log(1 - probs).T))\n",
    "    correct_logprobs = - (y* np.log(probs) + (1 - y) *np.log(1 - probs))\n",
    "    #print (correct_logprobs)\n",
    "    #correct_logprobs = -np.log(probs[range(num_examples), y]) \n",
    "    data_loss = np.sum(correct_logprobs)\n",
    "    print (data_loss)\n",
    "    # Add regulatization term to loss (optional)\n",
    "    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    \n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Function\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "For predictions, we will simply be using the forward propagation. No need to iterate or calculate the back propogation for supervised learning.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "\n",
    "def predict(model, x):\n",
    "    '''\n",
    "     Args:\n",
    "         model\n",
    "         x: input features\n",
    "    '''\n",
    "    # Extract weights and losses from the model\n",
    "    W1, W2, W3, W4, W5 = model['W1'], model['W2'], model['W3'], model['W4'], model['W5']\n",
    "    b1, b2, b3, b4, b5 = model['b1'], model['b2'], model['b3'], model['b4'], model['b5']\n",
    "    \n",
    "    #***********************************\n",
    "    # Layer 1\n",
    "    Z1 = x.dot(W1) + b1 \n",
    "    A1 = np.tanh(Z1)    # tanh activation\n",
    "    assert (Z1.shape == A1.shape),\"Shape of Z1 and A1 do not match\"\n",
    "\n",
    "    # Layer 2\n",
    "    Z2 = A1.dot(W2) + b2 \n",
    "    A2 = np.tanh(Z2)    # tanh activation\n",
    "    assert (Z2.shape == A2.shape),\"Shape of Z2 and A2 do not match\"\n",
    "\n",
    "    # Layer 3\n",
    "    Z3 = A2.dot(W3) + b3 \n",
    "    A3 = np.tanh(Z3)    # tanh activation\n",
    "    assert (Z3.shape == A3.shape),\"Shape of Z3 and A3 do not match\"\n",
    "\n",
    "    # Layer 4\n",
    "    Z4 = A3.dot(W4) + b4 \n",
    "    A4 = np.tanh(Z4)    # tanh activation\n",
    "    assert (Z4.shape == A4.shape),\"Shape of Z4 and A4 do not match\"\n",
    "\n",
    "\n",
    "    Z5 = A4.dot(W5) + b5\n",
    "    \n",
    "    # use softmax\n",
    "    exp_scores = np.exp(Z5) # softmax for final layer as it is binary classification\n",
    "    \n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    #*************************************    \n",
    "    return np.argmax(probs, axis=1) # pick with one with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the Model\n",
    "\n",
    "def build_model(param, X, y, num_epochs=20000,  print_loss=False):\n",
    "    \n",
    "    '''\n",
    "        nn_hdim : Number of nodes in the hidden layer\n",
    "        X : Features to train on\n",
    "        y : Targets to train on\n",
    "        num_passes : Number of passes through the training data for gradient descent\n",
    "        print_loss : If True, print the loss every 1000 iterations\n",
    "    '''\n",
    "    # set Random Seed\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    model_filename = os.path.join(outDir, modelfile)\n",
    "    exists = os.path.isfile(model_filename)\n",
    "    if exists:\n",
    "        print ('Model exists at : ', model_filename)\n",
    "        # Load configuration file values\n",
    "        infile = open(model_filename,'rb')\n",
    "        model = pickle.load(infile)\n",
    "        infile.close()\n",
    "        \n",
    "        # Initialise wwith loaded model\n",
    "        W1 = model['W1']\n",
    "        b1 = model['b1']\n",
    "        \n",
    "        W2 = model['W2']\n",
    "        b2 = model['b2']\n",
    "        \n",
    "        W3 = model['W3']\n",
    "        b3 = model['b3']\n",
    "        \n",
    "        W4 = model['W4']\n",
    "        b4 = model['b4']\n",
    "        \n",
    "        W5 = model['W5']\n",
    "        b5 = model['b5']\n",
    "        \n",
    "    else:\n",
    "        # Keep presets\n",
    "    \n",
    "        # Initialize the parameters to random model[values. We need to learn these.\n",
    "        W1 = np.random.randn(param['nn_hdim'][0], param['nn_hdim'][1]) / np.sqrt(param['nn_hdim'][0])\n",
    "        b1 = np.zeros((1, param['nn_hdim'][1]))\n",
    "\n",
    "        W2 = np.random.randn(param['nn_hdim'][1], param['nn_hdim'][2]) / np.sqrt(param['nn_hdim'][1])\n",
    "        b2 = np.zeros((1, param['nn_hdim'][2]))\n",
    "\n",
    "        W3 = np.random.randn(param['nn_hdim'][2], param['nn_hdim'][3]) / np.sqrt(param['nn_hdim'][2])\n",
    "        b3 = np.zeros((1, param['nn_hdim'][3]))\n",
    "\n",
    "        W4 = np.random.randn(param['nn_hdim'][3], param['nn_hdim'][4]) / np.sqrt(param['nn_hdim'][3])\n",
    "        b4 = np.zeros((1, param['nn_hdim'][4]))\n",
    "\n",
    "        W5 = np.random.randn(param['nn_hdim'][4], nn_output_dim) / np.sqrt(param['nn_hdim'][4])\n",
    "        b5 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "        # This is what we return at the end\n",
    "        model = {}\n",
    "    \n",
    "    assert (W1.shape == tuple(param['nn_hdim'][0:2])), 'Incorrect shape of W1 :{}'.format(W1.shape)\n",
    "    \n",
    "    \n",
    "    epoch = []\n",
    "    loss = []        \n",
    "    \n",
    "    # Gradient descent. For each batch...\n",
    "    for i in range(0, num_epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        # Layer 1\n",
    "        Z1 = X.dot(W1) + b1 \n",
    "        A1 = np.tanh(Z1)    # tanh activation\n",
    "        assert (Z1.shape == A1.shape),\"Shape of Z1 and A1 do not match\"\n",
    "        \n",
    "        # Layer 2\n",
    "        Z2 = A1.dot(W2) + b2 \n",
    "        A2 = np.tanh(Z2)    # tanh activation\n",
    "        assert (Z2.shape == A2.shape),\"Shape of Z2 and A2 do not match\"\n",
    "        \n",
    "        # Layer 3\n",
    "        Z3 = A2.dot(W3) + b3 \n",
    "        A3 = np.tanh(Z3)    # tanh activation\n",
    "        assert (Z3.shape == A3.shape),\"Shape of Z3 and A3 do not match\"\n",
    "\n",
    "        # Layer 4\n",
    "        Z4 = A3.dot(W4) + b4 \n",
    "        A4 = np.tanh(Z4)    # tanh activation\n",
    "        assert (Z4.shape == A4.shape),\"Shape of Z4 and A4 do not match\"\n",
    "\n",
    "\n",
    "        Z5 = A4.dot(W5) + b5\n",
    "        exp_scores = np.exp(Z5) # softmax for final layer as it is binary classification\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        #print ('probs.shape', probs.shape)\n",
    "\n",
    "        \n",
    "        # Backpropagation\n",
    "        assert (probs.shape == y.shape),\"Shape of probs {} and y {} do not match\".format(probs.shape, y.shape)\n",
    "        dZ5 = probs -y # shape = 1024, 2\n",
    "        # layer 5 # dZ = (a-y)\n",
    "        \n",
    "        dW5 = (A4.T).dot(dZ5)/num_examples \n",
    "        db5 = np.sum(dZ5, axis=0, keepdims=True) / num_examples # db5 is vertical sum of dZ5\n",
    "        dA4 = dZ5.dot(W5.T)\n",
    "        assert (dW5.shape == W5.shape),\"Shape of dW5 {} and W5 {} do not match\".format(dW5.shape, W5.shape)\n",
    "        assert (dA4.shape == A4.shape),\"Shape of dA4 {} and A4 {} do not match\".format(dA4.shape, A4.shape)\n",
    "        \n",
    "        # Layer 4 **********************************\n",
    "        dZ4 = dA4 * (1 - np.power(A4, 2))\n",
    "        assert (dZ4.shape == Z4.shape),\"Shape of dZ4 {} and Z4{} do not match\".format(dZ4.shape, Z4.shape)\n",
    "        \n",
    "        dW4 = (A3.T).dot(dZ4)/num_examples\n",
    "        assert (dW4.shape == W4.shape),\"Shape of dW4 {} and W4 {} do not match\".format(dW4.shape, W4.shape)\n",
    "        \n",
    "        db4 = np.sum(dZ4, axis=0, keepdims=True) / num_examples \n",
    "        dA3= dZ4.dot(W4.T)\n",
    "        assert (dA3.shape == A3.shape),\"Shape of dA3 {} and A3 {} do not match\".format(dA3.shape, A3.shape)\n",
    "\n",
    "        # Layer 3 **********************************\n",
    "        dZ3 = dA3 * (1 - np.power(A3, 2))\n",
    "        assert (dZ3.shape == Z3.shape),\"Shape of dZ3 {} and Z3{} do not match\".format(dZ3.shape, Z3.shape)\n",
    "        \n",
    "        dW3 = (A2.T).dot(dZ3)/num_examples\n",
    "        assert (dW3.shape == W3.shape),\"Shape of dW3 {} and W3 {} do not match\".format(dW3.shape, W3.shape)\n",
    "        \n",
    "        db3 = np.sum(dZ3, axis=0, keepdims=True) / num_examples \n",
    "        dA2= dZ3.dot(W3.T)\n",
    "        assert (dA2.shape == A2.shape),\"Shape of dA2 {} and A2 {} do not match\".format(dA2.shape, A2.shape)\n",
    "\n",
    "        # Layer 2 **********************************\n",
    "        dZ2 = dA2 * (1 - np.power(A2, 2))\n",
    "        assert (dZ2.shape == Z2.shape),\"Shape of dZ2 {} and Z2{} do not match\".format(dZ2.shape, Z2.shape)\n",
    "        \n",
    "        dW2 = (A1.T).dot(dZ2)/num_examples\n",
    "        assert (dW2.shape == W2.shape),\"Shape of dW2 {} and W2 {} do not match\".format(dW2.shape, W2.shape)\n",
    "        \n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / num_examples \n",
    "        dA1= dZ2.dot(W2.T)\n",
    "        assert (dA1.shape == A1.shape),\"Shape of dA1 {} and A1 {} do not match\".format(dA1.shape, A1.shape)\n",
    "\n",
    "        # Layer 1 **********************************\n",
    "        dZ1 = dA1 * (1 - np.power(A1, 2))\n",
    "        assert (dZ1.shape == Z1.shape),\"Shape of dZ1 {} and Z1{} do not match\".format(dZ1.shape, Z1.shape)\n",
    "        \n",
    "        dW1 = (X.T).dot(dZ1)/num_examples\n",
    "        assert (dW1.shape == W1.shape),\"Shape of dW2 {} and W2 {} do not match\".format(dW2.shape, W2.shape)\n",
    "        \n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / num_examples\n",
    "        assert (db1.shape == b1.shape),\"Shape of db1 {} and b1 {} do not match\".format(db1.shape, b1.shape)\n",
    "        #dA0= dZ1.dot(W1.T)\n",
    "        #assert (dA1.shape == A1.shape),\"Shape of dA1 {} and A1 {} do not match\".format(dA1.shape, A1.shape)\n",
    "\n",
    "\n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dW5 += reg_lambda * W5\n",
    "        dW4 += reg_lambda * W4\n",
    "        dW3 += reg_lambda * W3\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        \n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        \n",
    "        W3 += -epsilon * dW3\n",
    "        b3 += -epsilon * db3\n",
    "        \n",
    "        W4 += -epsilon * dW4\n",
    "        b4 += -epsilon * db4\n",
    "        \n",
    "        W5 += -epsilon * dW5\n",
    "        b5 += -epsilon * db5\n",
    "        \n",
    "        # Print the loss.\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            ls1 = calculate_loss(model, X, y)\n",
    "            print(\"Loss after iteration %i: %f\" %(i, ls1))\n",
    "            epoch.append(i)\n",
    "            loss.append(ls1)\n",
    "    \n",
    "    # Add for last Epoch\n",
    "    ls1 = calculate_loss(model, X, y)\n",
    "    print(\"Loss after iteration %i: %f\" %(i, ls1))\n",
    "    epoch.append(i)\n",
    "    loss.append(ls1)\n",
    "    \n",
    "    # Assign new parameters to the model\n",
    "    \n",
    "    model = {\n",
    "        'W1': W1, 'b1': b1,\n",
    "        'W2': W2, 'b2': b2,\n",
    "        'W3': W3, 'b3': b3,\n",
    "        'W4': W4, 'b4': b4,\n",
    "        'W5': W5, 'b5': b5,}\n",
    "    \n",
    "    loss_hist['epoch'] = epoch\n",
    "    loss_hist['loss'] = loss\n",
    "    \n",
    "    f = open(\"../output/model.pkl\",\"wb\")\n",
    "    pickle.dump(model,f)\n",
    "    f.close()\n",
    "    return model, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define nodes in each of dims\n",
    "layer_param = {}\n",
    "loss_hist = {}\n",
    "layer_param['nn_hdim'] = [X_train.shape[1],20,20,10,10,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(X_train) # training set size\n",
    "#nn_input_dim = 2 # input layer dimensionality\n",
    "nn_output_dim = 10 # output layer dimensionality\n",
    "\n",
    "# Gradient descent parameters\n",
    "epsilon = 0.1 # learning rate for gradient descent\n",
    "reg_lambda = 0.0 # regularization strength\n",
    "\n",
    "EPOCHS = 10000\n",
    "\n",
    "# Build a model with a multiple hidden layer\n",
    "model, probs = build_model(layer_param, X_train, y_train, num_epochs = EPOCHS, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_predicitions(pred_func, X):\n",
    "    y_pred = pred_func(X)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fn_make_predicitions(lambda x: predict(model, x), X_train)\n",
    "print('Accruacy score on Train Data :', accuracy_score(y_train.argmax(axis = 1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fn_make_predicitions(lambda x: predict(model, x), X_test)\n",
    "\n",
    "print('Accruacy score on Test Data :', accuracy_score(y_test.argmax(axis = 1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test.argmax(axis = 1), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(loss_hist)\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = loss_df.plot(x = 'epoch')\n",
    "ax.grid()\n",
    "ax.set_title('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
