{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 7 - With Dropouts and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "By Pramod Sharma : pramod.sharma@prasami.com\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import some libraries\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic parameters\n",
    "\n",
    "inpDir = '../input' # location where input data is stored\n",
    "outDir = '../output' # location to store outputs\n",
    "RANDOM_STATE = 24 # for initialization ----- REMEMBER: to remove at the time of promotion to production\n",
    "EPOCHS = 20000 # number of cycles to run\n",
    "\n",
    "# Set parameters for decoration of plots\n",
    "params = {'legend.fontsize' : 'large',\n",
    "          'figure.figsize'  : (9,6),\n",
    "          'axes.labelsize'  : 'x-large',\n",
    "          'axes.titlesize'  :'x-large',\n",
    "          'xtick.labelsize' :'large',\n",
    "          'ytick.labelsize' :'large',\n",
    "         }\n",
    "\n",
    "plt.rcParams.update(params) # update rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data Set\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "Lets use Sklearn's dataset generator. To keep the example simple, lets use  <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html\">make_moon</a> dataset generator.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=1280, shuffle=True, noise=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.1em;color:brown;\">\n",
    "<strong>Note</strong>: All two dimentional matrix are represented by Caps and all arrays (vectors) are represented by small case.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Plot the data\n",
    "plt.scatter(X[:,0], X[:,1], s=30, c=y, cmap=plt.cm.Spectral)\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_decision_boundary(pred_func, X, y):\n",
    "    '''\n",
    "        Attribite\n",
    "           pred_func : function based on predict method of the classifier\n",
    "       \n",
    "       Return:\n",
    "           None           \n",
    "    '''\n",
    "    \n",
    "    # Set min and max values and give it some padding\n",
    "    xMin, xMax = X[:, 0].min() - .05, X[:, 0].max() + .05\n",
    "    yMin, yMax = X[:, 1].min() - .05, X[:, 1].max() + .05\n",
    "    \n",
    "    # grid size for mesh grid\n",
    "    h = 0.01\n",
    "    \n",
    "    # Generate a grid of points with distance 'h' between them\n",
    "    xx, yy = np.meshgrid(np.arange(xMin, xMax, h), np.arange(yMin, yMax, h))\n",
    "    \n",
    "    # Predict the function value for the whole grid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Make its shape same as that of xx \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Now we have Z value corresponding to each of the combination of xx and yy\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    \n",
    "    # plot the points as well\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Split the data in training and test sets to measure performance of the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE )\n",
    "\n",
    "print (X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\"> \n",
    "    One of the method adopted for avoiding overfitting problem is <strong>DropOut</strong>:</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../Presentations/images/S7/nn_S7_f1.jpg' style='width: 800px'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate the total loss on the dataset\n",
    "\n",
    "def calculate_loss(model, X, y):\n",
    "    \n",
    "    # Extract weights and losses from the model\n",
    "    W1, W2, W3, W4, W5 = model['W1'], model['W2'], model['W3'], model['W4'], model['W5']\n",
    "    b1, b2, b3, b4, b5 = model['b1'], model['b2'], model['b3'], model['b4'], model['b5']\n",
    "    \n",
    "    #***********************************\n",
    "    # Layer 1\n",
    "    Z1 = X.dot(W1) + b1 \n",
    "    A1 = np.tanh(Z1)    # tanh activation\n",
    "    assert (Z1.shape == A1.shape),\"Shape of Z1 and A1 do not match\"\n",
    "\n",
    "    # Layer 2\n",
    "    Z2 = A1.dot(W2) + b2 \n",
    "    A2 = np.tanh(Z2)    # tanh activation\n",
    "    assert (Z2.shape == A2.shape),\"Shape of Z2 and A2 do not match\"\n",
    "\n",
    "    # Layer 3\n",
    "    Z3 = A2.dot(W3) + b3 \n",
    "    A3 = np.tanh(Z3)    # tanh activation\n",
    "    assert (Z3.shape == A3.shape),\"Shape of Z3 and A3 do not match\"\n",
    "\n",
    "    # Layer 4\n",
    "    Z4 = A3.dot(W4) + b4 \n",
    "    A4 = np.tanh(Z4)    # tanh activation\n",
    "    assert (Z4.shape == A4.shape),\"Shape of Z4 and A4 do not match\"\n",
    "\n",
    "\n",
    "    Z5 = A4.dot(W5) + b5\n",
    "    exp_scores = np.exp(Z5) # softmax for final layer as it is binary classification\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    #*************************************\n",
    "        \n",
    "    # Calculating the loss\n",
    "    # Cross entropy = ground truth x log (predicted) \n",
    "    #correct_logprobs = - (y* np.log(probs) + (1 - y) *np.log(1 - probs))\n",
    "    correct_logprobs = -np.log(probs[range(num_examples), y])  # y is still [1,1,0,1,0,1,1...]\n",
    "    data_loss = np.sum(correct_logprobs)\n",
    "    \n",
    "    # Add regulatization term to loss\n",
    "    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) +\n",
    "                                 np.sum(np.square(W2)) + \n",
    "                                 np.sum(np.square(W3)) +\n",
    "                                 np.sum(np.square(W4)) +\n",
    "                                 np.sum(np.square(W5)))\n",
    "    \n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "\n",
    "def predict(model, x):\n",
    "    '''\n",
    "     Args:\n",
    "         model\n",
    "         x: input features\n",
    "    '''\n",
    "    # Extract weights and losses from the model\n",
    "    W1, W2, W3, W4, W5 = model['W1'], model['W2'], model['W3'], model['W4'], model['W5']\n",
    "    b1, b2, b3, b4, b5 = model['b1'], model['b2'], model['b3'], model['b4'], model['b5']\n",
    "    \n",
    "    #***********************************\n",
    "    # Layer 1\n",
    "    Z1 = x.dot(W1) + b1 \n",
    "    A1 = np.tanh(Z1)    # tanh activation\n",
    "    assert (Z1.shape == A1.shape),\"Shape of Z1 and A1 do not match\"\n",
    "\n",
    "    # Layer 2\n",
    "    Z2 = A1.dot(W2) + b2 \n",
    "    A2 = np.tanh(Z2)    # tanh activation\n",
    "    assert (Z2.shape == A2.shape),\"Shape of Z2 and A2 do not match\"\n",
    "\n",
    "    # Layer 3\n",
    "    Z3 = A2.dot(W3) + b3 \n",
    "    A3 = np.tanh(Z3)    # tanh activation\n",
    "    assert (Z3.shape == A3.shape),\"Shape of Z3 and A3 do not match\"\n",
    "\n",
    "    # Layer 4\n",
    "    Z4 = A3.dot(W4) + b4 \n",
    "    A4 = np.tanh(Z4)    # tanh activation\n",
    "    assert (Z4.shape == A4.shape),\"Shape of Z4 and A4 do not match\"\n",
    "\n",
    "\n",
    "    Z5 = A4.dot(W5) + b5\n",
    "    \n",
    "    # use softmax\n",
    "    exp_scores = np.exp(Z5) # softmax for final layer as it is binary classification\n",
    "    \n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    #*************************************    \n",
    "    return np.argmax(probs, axis=1) # pick with one with highest probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropogation\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\partial{Z^{[2]}}  & = A^{[2]} - Y \\\\\n",
    "\\partial{W^{[2]}}  & = \\frac{1}{m} A^{[1]T}\\circ \\partial{Z^{[2]}} \\\\\n",
    "\\partial{b^{[2]}}  & = \\frac{1}{m} \\mathrm{np.sum}(\\partial{Z^{[2]}}, axis = 1, keepdims = True) \\\\\n",
    "\\\\\n",
    "\\partial{Z^{[1]}}  & = \\partial{Z^{[2]}}\\circ  W^{[2]T} * ( 1-A^{[1]}**2)\\\\\n",
    "\\partial{W^{[1]}}  & = \\frac{1}{m} X^{T}\\circ \\partial{Z^{[1]}} \\\\\n",
    "\\partial{b^{[1]}}  & = \\frac{1}{m} \\mathrm{np.sum}(\\partial{Z^{[1]}}, axis = 1, keepdims = True) \\\\\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_out_forward(A, keep_prob):\n",
    "    '''\n",
    "    Args:\n",
    "        A: A matrix\n",
    "        keep_prob: probabiity of keeping the nodes 0 < keep_prob < 1\n",
    "        cache : to keep all parameters to be cached\n",
    "    '''\n",
    "    # Implementing drop-outs\n",
    "    if keep_prob > 0:\n",
    "        drop_out = np.random.rand(A.shape[0], A.shape[1])\n",
    "        drop_out = (drop_out < keep_prob).astype(np.float)\n",
    "        A *= drop_out # makes the nodes 0.0\n",
    "        A /= keep_prob # Adjustment so that the weights do not over inflate\n",
    "    return A, drop_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_out_back(dA, drop_out, keep_prob):\n",
    "    \n",
    "    # Implementing drop-outs\n",
    "    if keep_prob > 0:\n",
    "        dA *= drop_out\n",
    "        dA /= keep_prob\n",
    "    return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the Model\n",
    "\n",
    "def build_model(param, X, y, num_passes=20000,  print_loss=False):\n",
    "    \n",
    "    '''\n",
    "        nn_hdim : Number of nodes in the hidden layer\n",
    "        X : Features to train on\n",
    "        y : Targets to train on\n",
    "        num_passes : Number of passes through the training data for gradient descent\n",
    "        print_loss : If True, print the loss every 1000 iterations\n",
    "    '''\n",
    "    # set Random Seed\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    \n",
    "    cache = {} # to retain all values to be cached\n",
    "    \n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    W1 = np.random.randn(param['nn_hdim'][0], param['nn_hdim'][1]) / np.sqrt(param['nn_hdim'][0])\n",
    "    b1 = np.zeros((1, param['nn_hdim'][1]))\n",
    "    \n",
    "    W2 = np.random.randn(param['nn_hdim'][1], param['nn_hdim'][2]) / np.sqrt(param['nn_hdim'][1])\n",
    "    b2 = np.zeros((1, param['nn_hdim'][2]))\n",
    "    \n",
    "    W3 = np.random.randn(param['nn_hdim'][2], param['nn_hdim'][3]) / np.sqrt(param['nn_hdim'][2])\n",
    "    b3 = np.zeros((1, param['nn_hdim'][3]))\n",
    "    \n",
    "    W4 = np.random.randn(param['nn_hdim'][3], param['nn_hdim'][4]) / np.sqrt(param['nn_hdim'][3])\n",
    "    b4 = np.zeros((1, param['nn_hdim'][4]))\n",
    "   \n",
    "    W5 = np.random.randn(param['nn_hdim'][4], nn_output_dim) / np.sqrt(param['nn_hdim'][4])\n",
    "    b5 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "    \n",
    "    assert (W1.shape == tuple(param['nn_hdim'][0:2])), 'Incorrect shape of W1 :{}'.format(W1.shape)\n",
    "    \n",
    "    epoch = []\n",
    "    loss = []\n",
    "    \n",
    "    # Gradient descent. For each batch...\n",
    "    for i in range(0, num_passes):\n",
    "\n",
    "        # Forward propagation\n",
    "        # Layer 1\n",
    "        Z1 = X.dot(W1) + b1 \n",
    "        A1 = np.tanh(Z1)    # tanh activation\n",
    "        A1, drop_out1 = drop_out_forward(A1, keep_prob)\n",
    "        cache['a1_dropout'] = drop_out1\n",
    "        assert (Z1.shape == A1.shape),\"Shape of Z1 and A1 do not match\"\n",
    "        \n",
    "        # Layer 2\n",
    "        Z2 = A1.dot(W2) + b2 \n",
    "        A2 = np.tanh(Z2)    # tanh activation\n",
    "        \n",
    "        A2, drop_out2 = drop_out_forward(A2, keep_prob) # Dropout\n",
    "        cache['a2_dropout'] = drop_out2\n",
    "        \n",
    "        assert (Z2.shape == A2.shape),\"Shape of Z2 and A2 do not match\"\n",
    "        \n",
    "        # Layer 3\n",
    "        Z3 = A2.dot(W3) + b3 \n",
    "        A3 = np.tanh(Z3)    # tanh activation\n",
    "        \n",
    "        A3,drop_out3 = drop_out_forward(A3, keep_prob) # Dropout\n",
    "        cache['a3_dropout'] = drop_out3\n",
    "        \n",
    "        assert (Z3.shape == A3.shape),\"Shape of Z3 and A3 do not match\"\n",
    "\n",
    "        # Layer 4\n",
    "        Z4 = A3.dot(W4) + b4 \n",
    "        A4 = np.tanh(Z4)    # tanh activation\n",
    "        \n",
    "        A4, drop_out4 = drop_out_forward(A4, keep_prob) # Dropout\n",
    "        cache['a4_dropout'] = drop_out4\n",
    "        \n",
    "        assert (Z4.shape == A4.shape),\"Shape of Z4 and A4 do not match\"\n",
    "\n",
    "\n",
    "        Z5 = A4.dot(W5) + b5\n",
    "        exp_scores = np.exp(Z5) # softmax for final layer as it is binary classification\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        #print ('probs.shape', probs.shape)\n",
    "\n",
    "        \n",
    "        # Backpropagation\n",
    "        delta5 = probs # shape = 1024, 2\n",
    "        delta5[range(num_examples), y] -= 1 # dL/db = dL/dz = (a-y). \n",
    "        #  As Y is single dimension subtract one from its class\n",
    "        \n",
    "        # layer 5 # dZ = (a-y) = delta5\n",
    "        dW5 = (A4.T).dot(delta5) / num_examples \n",
    "        db5 = np.sum(delta5, axis=0, keepdims=True) / num_examples # db5 is vertical sum of delta5\n",
    "        dA4 = delta5.dot(W5.T)\n",
    "        \n",
    "        dA4 = drop_out_back(dA4, cache['a4_dropout'], keep_prob) ## drop_layer\n",
    "        \n",
    "        assert (dW5.shape == W5.shape),\"Shape of dW5 {} and W5 {} do not match\".format(dW5.shape, W5.shape)\n",
    "        assert (dA4.shape == A4.shape),\"Shape of dA4 {} and A4 {} do not match\".format(dA4.shape, A4.shape)\n",
    "        \n",
    "        # Layer 4 **********************************\n",
    "        dZ4 = dA4 * (1 - np.power(A4, 2))\n",
    "        assert (dZ4.shape == Z4.shape),\"Shape of dZ4 {} and Z4{} do not match\".format(dZ4.shape, Z4.shape)\n",
    "        \n",
    "        dW4 = (A3.T).dot(dZ4)/num_examples\n",
    "        assert (dW4.shape == W4.shape),\"Shape of dW4 {} and W4 {} do not match\".format(dW4.shape, W4.shape)\n",
    "        \n",
    "        db4 = np.sum(dZ4, axis=0, keepdims=True) / num_examples \n",
    "        dA3= dZ4.dot(W4.T)\n",
    "        \n",
    "        dA3 = drop_out_back(dA3, cache['a3_dropout'], keep_prob) ## drop_layer\n",
    "        \n",
    "        assert (dA3.shape == A3.shape),\"Shape of dA3 {} and A3 {} do not match\".format(dA3.shape, A3.shape)\n",
    "\n",
    "        # Layer 3 **********************************\n",
    "        dZ3 = dA3 * (1 - np.power(A3, 2))\n",
    "        assert (dZ3.shape == Z3.shape),\"Shape of dZ3 {} and Z3{} do not match\".format(dZ3.shape, Z3.shape)\n",
    "        \n",
    "        dW3 = (A2.T).dot(dZ3)/num_examples\n",
    "        assert (dW3.shape == W3.shape),\"Shape of dW3 {} and W3 {} do not match\".format(dW3.shape, W3.shape)\n",
    "        \n",
    "        db3 = np.sum(dZ3, axis=0, keepdims=True) / num_examples \n",
    "        dA2= dZ3.dot(W3.T)\n",
    "        \n",
    "        dA2 = drop_out_back(dA2, cache['a2_dropout'], keep_prob) ## drop_layer\n",
    "        \n",
    "        assert (dA2.shape == A2.shape),\"Shape of dA2 {} and A2 {} do not match\".format(dA2.shape, A2.shape)\n",
    "\n",
    "        # Layer 2 **********************************\n",
    "        dZ2 = dA2 * (1 - np.power(A2, 2))\n",
    "        assert (dZ2.shape == Z2.shape),\"Shape of dZ2 {} and Z2{} do not match\".format(dZ2.shape, Z2.shape)\n",
    "        \n",
    "        dW2 = (A1.T).dot(dZ2)/num_examples\n",
    "        assert (dW2.shape == W2.shape),\"Shape of dW2 {} and W2 {} do not match\".format(dW2.shape, W2.shape)\n",
    "        \n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / num_examples \n",
    "        dA1= dZ2.dot(W2.T)\n",
    "        \n",
    "        dA1 = drop_out_back(dA1, cache['a1_dropout'], keep_prob) ## drop_layer\n",
    "        \n",
    "        assert (dA1.shape == A1.shape),\"Shape of dA1 {} and A1 {} do not match\".format(dA1.shape, A1.shape)\n",
    "\n",
    "        # Layer 1 **********************************\n",
    "        dZ1 = dA1 * (1 - np.power(A1, 2))\n",
    "        assert (dZ1.shape == Z1.shape),\"Shape of dZ1 {} and Z1{} do not match\".format(dZ1.shape, Z1.shape)\n",
    "        \n",
    "        dW1 = (X.T).dot(dZ1 )/ num_examples\n",
    "        assert (dW1.shape == W1.shape),\"Shape of dW2 {} and W2 {} do not match\".format(dW2.shape, W2.shape)\n",
    "        \n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / num_examples\n",
    "        assert (db1.shape == b1.shape),\"Shape of db1 {} and b1 {} do not match\".format(db1.shape, b1.shape)\n",
    "        #dA0= dZ1.dot(W1.T)\n",
    "        #assert (dA1.shape == A1.shape),\"Shape of dA1 {} and A1 {} do not match\".format(dA1.shape, A1.shape)\n",
    "\n",
    "\n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dW5 += reg_lambda * W5 / num_examples\n",
    "        dW4 += reg_lambda * W4 / num_examples\n",
    "        dW3 += reg_lambda * W3 / num_examples\n",
    "        dW2 += reg_lambda * W2 / num_examples\n",
    "        dW1 += reg_lambda * W1 / num_examples\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        \n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        \n",
    "        W3 += -epsilon * dW3\n",
    "        b3 += -epsilon * db3\n",
    "        \n",
    "        W4 += -epsilon * dW4\n",
    "        b4 += -epsilon * db4\n",
    "        \n",
    "        W5 += -epsilon * dW5\n",
    "        b5 += -epsilon * db5\n",
    "        \n",
    "        # Assign new parameters to the model\n",
    "        model = { \n",
    "            'W1': W1, 'b1': b1,\n",
    "            'W2': W2, 'b2': b2,\n",
    "            'W3': W3, 'b3': b3,\n",
    "            'W4': W4, 'b4': b4,\n",
    "            'W5': W5, 'b5': b5,}\n",
    "    \n",
    "        #Calculate loss after 100 epoch\n",
    "        if i % 100 == 0:\n",
    "            curr_loss = calculate_loss(model, X, y)\n",
    "            epoch.append(i)\n",
    "            loss.append(curr_loss)\n",
    "\n",
    "        \n",
    "        # Print the loss.\n",
    "        if print_loss and i % 5000 == 0:    \n",
    "            print(\"Loss after iteration %i: %f\" %(i, curr_loss))\n",
    "    \n",
    "    loss_hist['epoch'] = epoch\n",
    "    loss_hist['loss'] = loss\n",
    "    return model, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define nodes in each of dims\n",
    "layer_param = {}\n",
    "layer_param['nn_hdim'] = [X_train.shape[1],5,5,4,3,2]\n",
    "\n",
    "keep_prob = 0.95\n",
    "\n",
    "# lists to facilitate plotting \n",
    "loss_hist = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = X_train.shape[0] # training set size\n",
    "#nn_input_dim = 2 # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality\n",
    "\n",
    "# Gradient descent parameters\n",
    "epsilon = 0.01 # learning rate for gradient descent\n",
    "reg_lambda = 0.01 # regularization strength\n",
    "\n",
    "EPOCHS = 30000\n",
    "num_passes=EPOCHS\n",
    "\n",
    "# Build a model with a multiple layers\n",
    "model, probs = build_model(layer_param, X_train, y_train, num_passes = num_passes, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(loss_hist)\n",
    "\n",
    "fn_plot_decision_boundary(lambda x: predict(model, x), X_train, y_train) # plot decision boundary for this plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_predicitions(pred_func, X):\n",
    "    y_pred = pred_func(X)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fn_make_predicitions(lambda x: predict(model, x), X_train)\n",
    "print('Accruacy score on Train Data :', accuracy_score(y_train, y_pred))\n",
    "\n",
    "#Accruacy score on Train Data : 0.974609375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fn_make_predicitions(lambda x: predict(model, x), X_test)\n",
    "\n",
    "print('Accruacy score on Test Data :', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Accruacy score on Test Data : 0.95703125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2 , figsize = (15,6))\n",
    "\n",
    "l_range = 100\n",
    "\n",
    "ax = axes[0]\n",
    "\n",
    "loss_df.plot(x = 'epoch', y = 'loss', ax = ax)\n",
    "loss = loss_df['loss'].values\n",
    "\n",
    "# little beautification\n",
    "txtstr = \"Errors: \\n  Start : {:7.4f}\\n   End : {:7.4f}\".format(loss[0],loss[-1]) #text to plot\n",
    "# properties  matplotlib.patch.Patch \n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "\n",
    "ax.text(0.6, 0.95, txtstr, transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_title('Overall')\n",
    "ax.grid();\n",
    "\n",
    "ax = axes[1]\n",
    "\n",
    "loss_df[-l_range:].plot(x = 'epoch', y = 'loss', ax = ax)\n",
    "\n",
    "# little beautification\n",
    "txtstr = \"Errors: \\n  Start : {:7.4f}\\n   End : {:7.4f}\".format(loss[-l_range],loss[-1]) #text to plot\n",
    "# properties  matplotlib.patch.Patch \n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "\n",
    "ax.text(0.6, 0.95, txtstr, transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_title('Last {} records'.format(l_range))\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note on calculations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_try = np.random.randn(20).reshape(4,5)\n",
    "print(W_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_exact_dropout(W, keep_prob):\n",
    "    keep_count = np.int(np.ceil(W.shape[0] * W.shape[1] * keep_prob))\n",
    "    keep = np.random.choice(W_try.ravel(), size=keep_count, replace=False)\n",
    "    mask = np.in1d(W_try, keep).astype(np.float32).reshape(W.shape[0] ,W.shape[1] )\n",
    "    W *= mask\n",
    "    W /= keep_prob\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit W_exact = fn_exact_dropout(W_try, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit W_approx = drop_out_forward(W_try, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)\n",
    "layer = tf.keras.layers.Dropout(.2, input_shape=(4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit outputs = layer(W_try, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
