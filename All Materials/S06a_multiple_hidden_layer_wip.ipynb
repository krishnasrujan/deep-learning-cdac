{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6 : Multiple Hidden Layers, Target binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "By Pramod Sharma : pramod.sharma@prasami.com\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import some libraries\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic parameters\n",
    "\n",
    "inpDir = '../input' # location where input data is stored\n",
    "outDir = '../output' # location to store outputs\n",
    "RANDOM_STATE = 24 # for initialization ----- REMEMBER: to remove at the time of promotion to production\n",
    "EPOCHS = 20000 # number of cycles to run\n",
    "\n",
    "# Set parameters for decoration of plots\n",
    "params = {'legend.fontsize' : 'large',\n",
    "          'figure.figsize'  : (9,6),\n",
    "          'axes.labelsize'  : 'x-large',\n",
    "          'axes.titlesize'  :'x-large',\n",
    "          'xtick.labelsize' :'large',\n",
    "          'ytick.labelsize' :'large',\n",
    "         }\n",
    "\n",
    "plt.rcParams.update(params) # update rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data Set\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "Use Sklearn's dataset generator <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html\">make_moon</a> dataset generator.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=1280, shuffle=True, noise=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.1em;color:brown;\">\n",
    "<strong>Note</strong>: All two dimensional matrix are represented by Caps and all arrays (vectors) are represented by small case.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.scatter(X[:,0], X[:,1], s=30, c=y, cmap=plt.cm.Spectral)\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_decision_boundary(pred_func, X, y):\n",
    "    '''\n",
    "        Attrib:\n",
    "           pred_func : function based on predict method of the classifier\n",
    "           X : feature matrix\n",
    "           y : targets\n",
    "       Return:\n",
    "           None           \n",
    "    '''\n",
    "    \n",
    "    # Set min and max values and give it some padding\n",
    "    xMin, xMax = X[:, 0].min() - .05, X[:, 0].max() + .05\n",
    "    yMin, yMax = X[:, 1].min() - .05, X[:, 1].max() + .05\n",
    "    \n",
    "    # grid size for mesh grid\n",
    "    h = 0.01\n",
    "    \n",
    "    # Generate a grid of points with distance 'h' between them\n",
    "    xx, yy = np.meshgrid(np.arange(xMin, xMax, h), np.arange(yMin, yMax, h))\n",
    "    \n",
    "    # Predict the function value for the whole grid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Make its shape same as that of xx \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Now we have Z value corresponding to each of the combination of xx and yy\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    \n",
    "    # plot the points as well\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Split the data in training and test sets to measure performance of the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE )\n",
    "\n",
    "print (X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "## Recap\n",
    "<img src='../Presentations/images/S6/nn_S6_f1.jpg' style='width: 800px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The math..\n",
    "<img src='../Presentations/images/S6/nn_S6_f2.jpg' style='width: 800px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple hidden layers\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\"> \n",
    "    Moving over to multilayer network. Our data has <strong>two</strong> features. Hence size of input layer will also be two. The output is binary, we can code it as single column as well as double column output. The hidden layers will be as follows:</p>\n",
    "<table style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "    <tr>\n",
    "        <th>#</th>\n",
    "        <th>Layer Number</th>\n",
    "        <th>Nodes </th>\n",
    "        <th>Activation </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>Input Layer</td>\n",
    "        <td>2</td>\n",
    "        <td>tanh</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>Hidden Layer 1</td>\n",
    "        <td>5</td>\n",
    "        <td>tanh</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>Hidden Layer 2</td>\n",
    "        <td>5</td>\n",
    "        <td>tanh</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>4</td>\n",
    "        <td>Hidden Layer 3</td>\n",
    "        <td>4</td>\n",
    "        <td>tanh</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>5</td>\n",
    "        <td>Hidden Layer 4</td>\n",
    "        <td>3</td>\n",
    "        <td>tanh</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>6</td>\n",
    "        <td>Output Layer</td>\n",
    "        <td>2</td>\n",
    "        <td>softmax</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../Presentations/images/S6/nn_S6_f3.jpg' style='width: 800px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "The loss for our prediction $\\hat{y}$ with respect to the true labels $y$ is given by:\n",
    "</p>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\hat{y},y) =  -y.log\\hat{y} - (1-y) . log(1-\\hat{y})\n",
    "\\end{aligned}\n",
    "$$\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "For all samples:\n",
    "</p>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J(\\hat{y}, y) =  -\\frac{1}{m}\\sum_{i \\in m}y_i.log\\hat{y_i} - (1-y_i) . log(1-\\hat{y_i})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate the total loss on the dataset\n",
    "\n",
    "def calculate_loss(model, X, y):\n",
    "    \n",
    "    # Extract weights and losses from the model\n",
    "    W1, W2, W3, W4, W5 = model['W1'], model['W2'], model['W3'], model['W4'], model['W5']\n",
    "    b1, b2, b3, b4, b5 = model['b1'], model['b2'], model['b3'], model['b4'], model['b5']\n",
    "    \n",
    "    #***********************************\n",
    "    # Layer 1\n",
    "    Z1 = X.dot(W1) + b1 \n",
    "    A1 = np.tanh(Z1)    # tanh activation\n",
    "    assert (Z1.shape == A1.shape),\"Shape of Z1 and A1 do not match\"\n",
    "\n",
    "    # Layer 2\n",
    "    Z2 = A1.dot(W2) + b2 \n",
    "    A2 = np.tanh(Z2)    # tanh activation\n",
    "    assert (Z2.shape == A2.shape),\"Shape of Z2 and A2 do not match\"\n",
    "\n",
    "    # Layer 3\n",
    "    Z3 = A2.dot(W3) + b3 \n",
    "    A3 = np.tanh(Z3)    # tanh activation\n",
    "    assert (Z3.shape == A3.shape),\"Shape of Z3 and A3 do not match\"\n",
    "\n",
    "    # Layer 4\n",
    "    Z4 = A3.dot(W4) + b4 \n",
    "    A4 = np.tanh(Z4)    # tanh activation\n",
    "    assert (Z4.shape == A4.shape),\"Shape of Z4 and A4 do not match\"\n",
    "\n",
    "\n",
    "    Z5 = A4.dot(W5) + b5\n",
    "    exp_scores = np.exp(Z5) # softmax for final layer as it is binary classification\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    #*************************************\n",
    "        \n",
    "    # Calculating the loss\n",
    "    # Cross entropy = ground truth x log (predicted)\n",
    "    # probability of y being correct is 1. hence it will be a vector of [1,1,...,1,1]\n",
    "    \n",
    "    correct_logprobs = -np.log(probs[range(num_examples), y]) \n",
    "    data_loss = np.sum(correct_logprobs)\n",
    "    \n",
    "    # Add regulatization term to loss\n",
    "    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    \n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propogation\n",
    "\n",
    "<img src='../Presentations/images/S6/nn_S6_f4.jpg' style='width: 800px'/>\n",
    "$\n",
    "\\begin{aligned}\n",
    "Z^{[l]} & = A^{[l-1]} . W^{[l]} + b^{[l]}\\\\\n",
    "A^{[l]} & = \\tanh(Z^{[l]}) \\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "<hr>\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "    And for last layer.</p>\n",
    "$\n",
    "\\begin{aligned}\n",
    "Z^{[L]} & = A^{[L-1]} . W^{[L]} + b^{[L]} \\\\\n",
    "A^{[L]} & = \\mathrm{softmax}(Z^{[L]}) \\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "    Where:\n",
    "</p>\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathrm{softmax}(z_i) & =  \\frac{e^{z_i}}{\\sum_{i=1}^n {e^{z_i}}}\\\\\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Function\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "For predictions, we will simply be using the forward propagation. No need to iterate or calculate the back propogation for supervised learning.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "\n",
    "def predict(model, x):\n",
    "    '''\n",
    "     Args:\n",
    "         model\n",
    "         x: input features\n",
    "    '''\n",
    "    # Extract weights and losses from the model\n",
    "    W1, W2, W3, W4, W5 = model['W1'], model['W2'], model['W3'], model['W4'], model['W5']\n",
    "    b1, b2, b3, b4, b5 = model['b1'], model['b2'], model['b3'], model['b4'], model['b5']\n",
    "    \n",
    "    #***********************************\n",
    "    # Layer 1\n",
    "    Z1 = x.dot(W1) + b1 \n",
    "    A1 = np.tanh(Z1)    # tanh activation\n",
    "    assert (Z1.shape == A1.shape),\"Shape of Z1 and A1 do not match\"\n",
    "\n",
    "    # Layer 2\n",
    "    Z2 = A1.dot(W2) + b2 \n",
    "    A2 = np.tanh(Z2)    # tanh activation\n",
    "    assert (Z2.shape == A2.shape),\"Shape of Z2 and A2 do not match\"\n",
    "\n",
    "    # Layer 3\n",
    "    Z3 = A2.dot(W3) + b3 \n",
    "    A3 = np.tanh(Z3)    # tanh activation\n",
    "    assert (Z3.shape == A3.shape),\"Shape of Z3 and A3 do not match\"\n",
    "\n",
    "    # Layer 4\n",
    "    Z4 = A3.dot(W4) + b4 \n",
    "    A4 = np.tanh(Z4)    # tanh activation\n",
    "    assert (Z4.shape == A4.shape),\"Shape of Z4 and A4 do not match\"\n",
    "\n",
    "\n",
    "    Z5 = A4.dot(W5) + b5\n",
    "    \n",
    "    # use softmax\n",
    "    exp_scores = np.exp(Z5) # softmax for final layer as it is binary classification\n",
    "    \n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    #*************************************    \n",
    "    return np.argmax(probs, axis=1) # pick with one with highest probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "We can use $gradient$ $descent$ to find its minimum. For the purpose of this excercise, we will use $batch$ $gradient$ descent with a fixed learning rate. \n",
    "</p>\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "As an input, gradient descent needs the gradients (vector of derivatives) of the loss function with respect to our parameters: $\\frac{\\partial{L}}{\\partial{W_l}}(= \\partial{W^{[l]}})$, $\\frac{\\partial{L}}{\\partial{b_l}}(= \\partial{b^{[l]}})$, etc. To calculate these gradients we use the *backpropagation algorithm*, which is a way to efficiently calculate the gradients starting from the output.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropogation\n",
    "<hr>\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "    For last layer.</p>\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\partial{Z^{[L]}}  & = A^{[L]} - y \\\\\n",
    "\\partial{W^{[L]}}  & = \\frac{1}{m} A^{[L-1]T}\\circ \\partial{Z^{[L]}} \\\\\n",
    "\\partial{b^{[L]}}  & = \\frac{1}{m} \\mathrm{np.sum}(\\partial{Z^{[L]}}, axis = 1, keepdims = True) \\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "<hr>\n",
    "<p style=\"font-family: Arial; font-size:1.2em;color:black;\">\n",
    "    For any other layer</p>\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\partial{A^{[l]}}  & = \\partial{Z^{[l+1]}} . \\partial{W^{[l+1]T}}\\\\\n",
    "\\partial{Z^{[l]}}  & = \\partial{A^{[l]}} * ( 1-A^{[l]}**2)\\\\\n",
    "\\partial{W^{[l]}}  & = \\frac{1}{m} A^{[l-1]T}\\circ \\partial{Z^{[l]}} \\\\\n",
    "\\partial{b^{[l]}}  & = \\frac{1}{m} \\mathrm{np.sum}(\\partial{Z^{[l]}}, axis = 1, keepdims = True) \\\\\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the Model\n",
    "\n",
    "def build_model(param, X, y, num_passes=20000,  print_loss=False):\n",
    "    \n",
    "    '''\n",
    "        nn_hdim : Number of nodes in the hidden layer\n",
    "        X : Features to train on\n",
    "        y : Targets to train on\n",
    "        num_passes : Number of passes through the training data for gradient descent\n",
    "        print_loss : If True, print the loss every 1000 iterations\n",
    "    '''\n",
    "    # set Random Seed\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    \n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    W1 = np.random.randn(param['nn_hdim'][0], param['nn_hdim'][1]) / np.sqrt(param['nn_hdim'][0])\n",
    "    b1 = np.zeros((1, param['nn_hdim'][1]))\n",
    "    \n",
    "    W2 = np.random.randn(param['nn_hdim'][1], param['nn_hdim'][2]) / np.sqrt(param['nn_hdim'][1])\n",
    "    b2 = np.zeros((1, param['nn_hdim'][2]))\n",
    "    \n",
    "    W3 = np.random.randn(param['nn_hdim'][2], param['nn_hdim'][3]) / np.sqrt(param['nn_hdim'][2])\n",
    "    b3 = np.zeros((1, param['nn_hdim'][3]))\n",
    "    \n",
    "    W4 = np.random.randn(param['nn_hdim'][3], param['nn_hdim'][4]) / np.sqrt(param['nn_hdim'][3])\n",
    "    b4 = np.zeros((1, param['nn_hdim'][4]))\n",
    "   \n",
    "    W5 = np.random.randn(param['nn_hdim'][4], nn_output_dim) / np.sqrt(param['nn_hdim'][4])\n",
    "    b5 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "    \n",
    "    assert (W1.shape == tuple(param['nn_hdim'][0:2])), 'Incorrect shape of W1 :{}'.format(W1.shape)\n",
    "    \n",
    "    \n",
    "    # Gradient descent. For each batch...\n",
    "    for i in range(0, num_passes):\n",
    "\n",
    "        # Forward propagation\n",
    "        # Layer 1\n",
    "        Z1 = X.dot(W1) + b1 \n",
    "        A1 = np.tanh(Z1)    # tanh activation\n",
    "        assert (Z1.shape == A1.shape),\"Shape of Z1 and A1 do not match\"\n",
    "        \n",
    "        # Layer 2\n",
    "        Z2 = A1.dot(W2) + b2 \n",
    "        A2 = np.tanh(Z2)    # tanh activation\n",
    "        assert (Z2.shape == A2.shape),\"Shape of Z2 and A2 do not match\"\n",
    "        \n",
    "        # Layer 3\n",
    "        Z3 = A2.dot(W3) + b3 \n",
    "        A3 = np.tanh(Z3)    # tanh activation\n",
    "        assert (Z3.shape == A3.shape),\"Shape of Z3 and A3 do not match\"\n",
    "\n",
    "        # Layer 4\n",
    "        Z4 = A3.dot(W4) + b4 \n",
    "        A4 = np.tanh(Z4)    # tanh activation\n",
    "        assert (Z4.shape == A4.shape),\"Shape of Z4 and A4 do not match\"\n",
    "\n",
    "\n",
    "        Z5 = A4.dot(W5) + b5\n",
    "        exp_scores = np.exp(Z5) # softmax for final layer as it is binary classification\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        #print ('probs.shape', probs.shape)\n",
    "\n",
    "        #########################\n",
    "        #### Backpropagation #### \n",
    "        #########################\n",
    "        \n",
    "        # Layer 5 **********************************\n",
    "        dZ5 = probs # shape = 1024, 2\n",
    "        dZ5[range(num_examples), y] -= 1 # dL/db = dL/dz = (a-y). \n",
    "        #  As Y is single dimension subtract one from its class\n",
    "        \n",
    "        dW5 = (A4.T).dot(dZ5)/num_examples \n",
    "        db5 = np.sum(dZ5, axis=0, keepdims=True) / num_examples # db5 is vertical sum of delta5\n",
    "        dA4 = dZ5.dot(W5.T)\n",
    "        assert (dW5.shape == W5.shape),\"Shape of dW5 {} and W5 {} do not match\".format(dW5.shape, W5.shape)\n",
    "        assert (dA4.shape == A4.shape),\"Shape of dA4 {} and A4 {} do not match\".format(dA4.shape, A4.shape)\n",
    "        \n",
    "        # Layer 4 **********************************\n",
    "        dZ4 = dA4 * (1 - np.power(A4, 2))\n",
    "        assert (dZ4.shape == Z4.shape),\"Shape of dZ4 {} and Z4{} do not match\".format(dZ4.shape, Z4.shape)\n",
    "        \n",
    "        dW4 = (A3.T).dot(dZ4)/num_examples\n",
    "        assert (dW4.shape == W4.shape),\"Shape of dW4 {} and W4 {} do not match\".format(dW4.shape, W4.shape)\n",
    "        \n",
    "        db4 = np.sum(dZ4, axis=0, keepdims=True) / num_examples \n",
    "        dA3= dZ4.dot(W4.T)\n",
    "        assert (dA3.shape == A3.shape),\"Shape of dA3 {} and A3 {} do not match\".format(dA3.shape, A3.shape)\n",
    "\n",
    "        # Layer 3 **********************************\n",
    "        dZ3 = dA3 * (1 - np.power(A3, 2))\n",
    "        assert (dZ3.shape == Z3.shape),\"Shape of dZ3 {} and Z3{} do not match\".format(dZ3.shape, Z3.shape)\n",
    "        \n",
    "        dW3 = (A2.T).dot(dZ3)/num_examples\n",
    "        assert (dW3.shape == W3.shape),\"Shape of dW3 {} and W3 {} do not match\".format(dW3.shape, W3.shape)\n",
    "        \n",
    "        db3 = np.sum(dZ3, axis=0, keepdims=True) / num_examples \n",
    "        dA2= dZ3.dot(W3.T)\n",
    "        assert (dA2.shape == A2.shape),\"Shape of dA2 {} and A2 {} do not match\".format(dA2.shape, A2.shape)\n",
    "\n",
    "        # Layer 2 **********************************\n",
    "        dZ2 = dA2 * (1 - np.power(A2, 2))\n",
    "        assert (dZ2.shape == Z2.shape),\"Shape of dZ2 {} and Z2{} do not match\".format(dZ2.shape, Z2.shape)\n",
    "        \n",
    "        dW2 = (A1.T).dot(dZ2)/num_examples\n",
    "        assert (dW2.shape == W2.shape),\"Shape of dW2 {} and W2 {} do not match\".format(dW2.shape, W2.shape)\n",
    "        \n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / num_examples \n",
    "        dA1= dZ2.dot(W2.T)\n",
    "        assert (dA1.shape == A1.shape),\"Shape of dA1 {} and A1 {} do not match\".format(dA1.shape, A1.shape)\n",
    "\n",
    "        # Layer 1 **********************************\n",
    "        dZ1 = dA1 * (1 - np.power(A1, 2))\n",
    "        assert (dZ1.shape == Z1.shape),\"Shape of dZ1 {} and Z1{} do not match\".format(dZ1.shape, Z1.shape)\n",
    "        \n",
    "        dW1 = (X.T).dot(dZ1)/num_examples\n",
    "        assert (dW1.shape == W1.shape),\"Shape of dW2 {} and W2 {} do not match\".format(dW2.shape, W2.shape)\n",
    "        \n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / num_examples\n",
    "        assert (db1.shape == b1.shape),\"Shape of db1 {} and b1 {} do not match\".format(db1.shape, b1.shape)\n",
    "        #dA0= dZ1.dot(W1.T)\n",
    "        #assert (dA1.shape == A1.shape),\"Shape of dA1 {} and A1 {} do not match\".format(dA1.shape, A1.shape)\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        \n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        \n",
    "        W3 += -epsilon * dW3\n",
    "        b3 += -epsilon * db3\n",
    "        \n",
    "        W4 += -epsilon * dW4\n",
    "        b4 += -epsilon * db4\n",
    "        \n",
    "        W5 += -epsilon * dW5\n",
    "        b5 += -epsilon * db5\n",
    "        \n",
    "        # Assign new parameters to the model\n",
    "        model = { \n",
    "            'W1': W1, 'b1': b1,\n",
    "            'W2': W2, 'b2': b2,\n",
    "            'W3': W3, 'b3': b3,\n",
    "            'W4': W4, 'b4': b4,\n",
    "            'W5': W5, 'b5': b5,}\n",
    "        \n",
    "        \n",
    "        curr_loss = calculate_loss(model, X, y)\n",
    "        loss.append(curr_loss)\n",
    "        num_epoch.append(i)\n",
    "        \n",
    "        # Print the loss.\n",
    "        if print_loss and i % 5000 == 0:    \n",
    "            print(\"Loss after iteration %i: %f\" %(i, curr_loss))\n",
    "            \n",
    "    return model, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define nodes in each of dims\n",
    "layer_param = {}\n",
    "layer_param['nn_hdim'] = [2,5,5,4,3,2]\n",
    "\n",
    "# lists to facilitate plotting \n",
    "loss = []\n",
    "num_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(X_train) # training set size\n",
    "#nn_input_dim = 2 # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality\n",
    "\n",
    "# Gradient descent parameters\n",
    "epsilon = 0.1 # learning rate for gradient descent\n",
    "reg_lambda = 0.0 # regularization strength\n",
    "\n",
    "EPOCHS = 20000\n",
    "num_passes=EPOCHS\n",
    "\n",
    "# Build a model with a 4-dimensional hidden layer\n",
    "model, probs = build_model(layer_param, X_train, y_train, num_passes = num_passes, print_loss=True)\n",
    "\n",
    "fn_plot_decision_boundary(lambda x: predict(model, x), X_train, y_train) # plot decision boundary for this plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_make_predicitions(pred_func, X):\n",
    "    y_pred = pred_func(X)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fn_make_predicitions(lambda x: predict(model, x), X_train)\n",
    "print('Accruacy score on Test Data :', accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fn_make_predicitions(lambda x: predict(model, x), X_test)\n",
    "\n",
    "print('Accruacy score on Test Data :', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2 , figsize = (15,6))\n",
    "\n",
    "l_range = 10000\n",
    "\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(num_epoch[100:], loss[100:]); # for zooming and scaling ignore first two\n",
    "\n",
    "# little beautification\n",
    "txtstr = \"Errors: \\n  Start : {:7.4f}\\n   End : {:7.4f}\".format(loss[0],loss[-1]) #text to plot\n",
    "# properties  matplotlib.patch.Patch \n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "\n",
    "ax.text(0.6, 0.95, txtstr, transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_title('Overall')\n",
    "ax.grid();\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(num_epoch[-l_range:], loss[-l_range:]); # for zooming and scaling ignore first two\n",
    "\n",
    "# little beautification\n",
    "txtstr = \"Errors: \\n  Start : {:7.4f}\\n   End : {:7.4f}\".format(loss[-l_range],loss[-1]) #text to plot\n",
    "# properties  matplotlib.patch.Patch \n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "\n",
    "ax.text(0.6, 0.95, txtstr, transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_title('Last {} records'.format(l_range))\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
