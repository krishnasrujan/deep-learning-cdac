{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time \n",
    "\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,n_samples,layers,l_rate,reg_lambda,batch_size):\n",
    "        \n",
    "        self.m = batch_size                        # m is number of samples in the dataset\n",
    "        self.n = n_samples                        # n in number of independent features\n",
    "        \n",
    "        self.l_rate = l_rate                       # learning rate \n",
    "        self.reg_lambda = reg_lambda               # regularization parameter\n",
    "        \n",
    "        # Initializing weights for hidden layers\n",
    "        self.W1 = np.random.rand(layers[0],self.n)\n",
    "        self.b1 = np.random.rand(1,layers[0])\n",
    "        \n",
    "        self.W2 = np.random.rand(layers[1],layers[0])\n",
    "        self.b2 = np.random.rand(1,layers[1])\n",
    "        \n",
    "        self.W3 = np.random.rand(layers[2],layers[1])\n",
    "        self.b3 = np.random.rand(1,layers[2])\n",
    "        \n",
    "        self.model = {'epoch':[],'W1':[],'b1':[],'W2':[],'b2':[],'W3':[],'b3':[]}\n",
    "    \n",
    "    def update_model(self,epoch):\n",
    "        \n",
    "        self.model['epoch'].append(epoch)\n",
    "        self.model['W1'].append(self.W1)\n",
    "        self.model['b1'].append(self.b1)\n",
    "        self.model['W2'].append(self.W2)\n",
    "        self.model['b2'].append(self.b2)\n",
    "        self.model['W3'].append(self.W3)\n",
    "        self.model['b3'].append(self.b3)\n",
    "        \n",
    "    def tanh_derivative(self,a):\n",
    "        \n",
    "        return 1-np.power(a,2)\n",
    "    \n",
    "    def train(self,X,y):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # forward propagation\n",
    "        self.Z1 = np.dot(self.X,self.W1.T)+self.b1\n",
    "        self.A1 = np.tanh(self.Z1)\n",
    "        \n",
    "        assert(self.Z1.shape == self.A1.shape),'Z1 and A1 shapes not equal'\n",
    "        \n",
    "        self.Z2 = np.dot(self.A1,self.W2.T)+self.b2\n",
    "        self.A2 =  np.tanh(self.Z2)\n",
    "        \n",
    "        assert(self.Z2.shape == self.A2.shape),'Z2 and A2 shapes not equal'\n",
    "        \n",
    "        self.Z3 = np.dot(self.A2,self.W3.T)+self.b3\n",
    "        self.y_hat = softmax(self.Z3,axis=1)\n",
    "        \n",
    "        \n",
    "        # backward propagation\n",
    "        self.y = self.y.reshape(self.y_hat.shape) # reshaping y into y_hat's shape to avoid wrong calculations \n",
    "        \n",
    "        #layer 3\n",
    "        self.dZ3 = self.y_hat - self.y \n",
    "        \n",
    "        self.dW3 = (1/self.m) * np.dot(self.dZ3.T,self.A2)\n",
    "        \n",
    "        self.db3 = (1/self.m) * np.sum(self.dZ3,axis=0,keepdims=True)\n",
    "        \n",
    "        assert self.dW3.shape == self.W3.shape\n",
    "        assert self.db3.shape == self.b3.shape\n",
    "\n",
    "        #layer 2\n",
    "        self.dZ2 = np.multiply( np.dot(self.dZ3,self.W3) , self.tanh_derivative(self.A2) )\n",
    "        \n",
    "        assert self.dZ2.shape == self.A2.shape,\"dZ2 and A2 shapes not equal\"\n",
    "        \n",
    "        self.dW2 = (1/self.m) * np.dot(self.dZ2.T,self.A1)\n",
    "        \n",
    "        self.db2 = (1/self.m) * np.sum(self.dZ2,axis=0,keepdims=True)\n",
    "        \n",
    "        assert self.dW2.shape == self.W2.shape\n",
    "        assert self.db2.shape == self.b2.shape\n",
    "        \n",
    "        #layer 1\n",
    "        \n",
    "        self.dZ1 = np.multiply( np.dot(self.dZ2,self.W2) , self.tanh_derivative(self.A1) )\n",
    "        \n",
    "        self.dW1 = (1/self.m) * np.dot(self.dZ1.T,self.X)\n",
    "        \n",
    "        self.db1 = (1/self.m) * np.sum(self.dZ1,axis=0,keepdims=True)\n",
    "        \n",
    "        self.dW3 = self.dW3 + self.reg_lambda * self.dW3   # regularizing to reduce overfitting \n",
    "        self.db3 = self.db3 + self.reg_lambda * self.db3\n",
    "        self.dW2 = self.dW2 + self.reg_lambda * self.dW2   # regularizing to reduce overfitting \n",
    "        self.db2 = self.db2 + self.reg_lambda * self.db2\n",
    "        self.dW1 = self.dW1 + self.reg_lambda * self.dW1        \n",
    "        self.db2 = self.db2 + self.reg_lambda * self.db2\n",
    "        \n",
    "        self.W3 = self.W3 - self.l_rate * self.dW3 # updating weights\n",
    "        self.b3 = self.b3 - self.l_rate * self.db3 # updating bias\n",
    "        self.W2 = self.W2 - self.l_rate * self.dW2 # updating weights\n",
    "        self.b2 = self.b2 - self.l_rate * self.db2 # updating bias\n",
    "        self.W1 = self.W1 - self.l_rate * self.dW1 # updating weights\n",
    "        self.b1 = self.b1 - self.l_rate * self.db1 # updating bias\n",
    "        \n",
    "    def cost(self):\n",
    "\n",
    "        return (-1/self.m) * np.sum ((np.multiply(np.log(self.y_hat),self.y)))\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \n",
    "        Z1 = np.dot(x,self.W1.T) + self.b1 \n",
    "        \n",
    "        A1 = np.tanh(Z1)\n",
    "        \n",
    "        Z2 = np.dot(A1,self.W2.T) + self.b2\n",
    "        \n",
    "        A2 = np.tanh(Z2)\n",
    "        \n",
    "        Z3 = np.dot(A2,self.W3.T) + self.b3\n",
    "        \n",
    "        y_hat = softmax(Z3)  \n",
    "\n",
    "        preds = np.argmax(y_hat,axis=1)\n",
    "\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/Srujan/Documents/Datasets/fashion-mnist_train.csv')\n",
    "X = train.iloc[:,1:]\n",
    "y = train.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(sparse=False)\n",
    "y_transformed_train = onehot.fit_transform(y_train)\n",
    "y_transformed_test = onehot.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_transformed_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = y_transformed_train.shape[1]\n",
    "n_samples = X_train.shape[1]\n",
    "batch_size=32\n",
    "\n",
    "n = NeuralNetwork(n_samples,[3,3,n_classes],0.002,0.003,batch_size)\n",
    "cost_list = []\n",
    "\n",
    "for epoch in range(0,1000):\n",
    "    start = 0                                                #batch starting index\n",
    "    end = batch_size\n",
    "    while (end <= X_train.shape[0]):\n",
    "\n",
    "        x_trainable = X_train[start:end,:]                    \n",
    "        y_trainable = y_transformed_train[start:end,:]\n",
    "        n.train(x_trainable,y_trainable)                     # batch size training\n",
    "        start = start+batch_size\n",
    "        end = end+batch_size                                 # updating indexes of batch size \n",
    "    \n",
    "    cost_list.append(n.cost())\n",
    "    n.update_model(epoch)   \n",
    "    \n",
    "plt.figure(figsize=(12,4)) # plotting cost for every learning rate\n",
    "x_vals = list(range(1000))\n",
    "sns.lineplot(x=x_vals,y=cost_list)\n",
    "\n",
    "y_train_preds = n.predict(X_train)  # converting output of sigmoid into predictions\n",
    "y_test_preds = n.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train,y_train_preds)\n",
    "test_accuracy = accuracy_score(y_test,y_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('learning_rate', [0.01]), ('train_accuracy', [0.1]), ('test_accuracy', [0.1])])\n",
      "dict_items([('learning_rate', [0.01, 0.003]), ('train_accuracy', [0.1, 0.1]), ('test_accuracy', [0.1, 0.1])])\n",
      "dict_items([('learning_rate', [0.01, 0.003, 0.005]), ('train_accuracy', [0.1, 0.1, 0.1]), ('test_accuracy', [0.1, 0.1, 0.1])])\n",
      "dict_items([('learning_rate', [0.01, 0.003, 0.005, 0.001]), ('train_accuracy', [0.1, 0.1, 0.1, 0.1]), ('test_accuracy', [0.1, 0.1, 0.1, 0.1])])\n",
      "dict_items([('learning_rate', [0.01, 0.003, 0.005, 0.001, 0.0005]), ('train_accuracy', [0.1, 0.1, 0.1, 0.1, 0.1]), ('test_accuracy', [0.1, 0.1, 0.1, 0.1, 0.1])])\n",
      "Wall time: 22min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learning_rates = [0.01,0.003,0.005,0.001,0.0005]\n",
    "n_classes = y_transformed_train.shape[1]\n",
    "n_samples = X_train.shape[1]\n",
    "\n",
    "batch_size=64\n",
    "metrics = {'learning_rate':[],'train_accuracy':[],'test_accuracy':[]}\n",
    "\n",
    "for lr in learning_rates: \n",
    "    \n",
    "    n = NeuralNetwork(n_samples,[3,3,n_classes],lr,0.003,batch_size)\n",
    "\n",
    "    for epoch in range(0,500):\n",
    "        start = 0                                                #batch starting index\n",
    "        end = batch_size\n",
    "        while (end <= X_train.shape[0]):\n",
    "\n",
    "            x_trainable = X_train[start:end,:]                    \n",
    "            y_trainable = y_transformed_train[start:end,:]\n",
    "            n.train(x_trainable,y_trainable)                     # batch size training\n",
    "            start = start+batch_size\n",
    "            end = end+batch_size                                 # updating indexes of batch size \n",
    "\n",
    "        n.update_model(epoch)   \n",
    "\n",
    "    y_train_preds = n.predict(X_train)  # converting output of sigmoid into predictions\n",
    "    y_test_preds = n.predict(X_test)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train,y_train_preds)\n",
    "    test_accuracy = accuracy_score(y_test,y_test_preds)\n",
    "    #adding to dictionary\n",
    "    metrics['learning_rate'].append(lr)\n",
    "    metrics['train_accuracy'].append(train_accuracy)\n",
    "    metrics['test_accuracy'].append(test_accuracy)   \n",
    "    print(metrics.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01,0.003,0.005,0.001,0.0005]\n",
    "n_classes = y_transformed_train.shape[1]\n",
    "n_samples = X_train.shape[1]\n",
    "metrics = {'learning_rate':[],'nodes':[],'reg_param':[],'batch_size':[],'train_accuracy':[],'test_accuracy':[]}\n",
    "\n",
    "for lr in learning_rates: # tuning learning rate\n",
    "    \n",
    "    # tuning nodes in hidden layers\n",
    "    for nodes in [[2,3,n_classes],[3,3,n_classes],[4,3,n_classes],[3,4,n_classes],[4,5,n_classes],[4,4,n_classes]]:\n",
    "        \n",
    "        for reg_param in [0.003,0.005,0.01]: # tuning regularization parameter\n",
    "            \n",
    "            for batch_size in [32,62,128]: # tuning batch size\n",
    "            \n",
    "                n = NeuralNetwork(n_samples,nodes,lr,reg_param,batch_size)\n",
    "                \n",
    "                for epoch in range(0,500):\n",
    "                    start = 0                                                #batch starting index\n",
    "                    end = batch_size\n",
    "                    while (end <= X_train.shape[0]):\n",
    "\n",
    "                        x_trainable = X_train[start:end,:]                    \n",
    "                        y_trainable = y_transformed_train[start:end,:]\n",
    "                        n.train(x_trainable,y_trainable)                     # batch size training\n",
    "                        start = start+batch_size\n",
    "                        end = end+batch_size                                 # updating indexes of batch size \n",
    "                    \n",
    "                    n.update_model(epoch)   \n",
    "                    \n",
    "                y_train_preds = n.predict(X_train)  # converting output of sigmoid into predictions\n",
    "                y_test_preds = n.predict(X_test)\n",
    "\n",
    "                train_accuracy = accuracy_score(y_train,y_train_preds)\n",
    "                test_accuracy = accuracy_score(y_test,y_test_preds)\n",
    "                \n",
    "                # adding to dictionary\n",
    "                metrics['learning_rate'].append(lr)\n",
    "                metrics['nodes'].append(nodes)\n",
    "                metrics['reg_param'].append(reg_param)\n",
    "                metrics['batch_size'].append(batch_size)\n",
    "                metrics['train_accuracy'].append(train_accuracy)\n",
    "                metrics['test_accuracy'].append(test_accuracy)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(metrics)\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
