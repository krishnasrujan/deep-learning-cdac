{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time \n",
    "\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,n_samples,layers,l_rate,reg_lambda,batch_size):\n",
    "        \n",
    "        self.m = batch_size                        # m is number of samples in the dataset\n",
    "        self.n = n_samples                        # n in number of independent features\n",
    "        \n",
    "        self.l_rate = l_rate                       # learning rate \n",
    "        self.reg_lambda = reg_lambda               # regularization parameter\n",
    "        \n",
    "        # Initializing weights for hidden layers\n",
    "        self.W1 = np.random.rand(layers[0],self.n)\n",
    "        self.b1 = np.random.rand(1,layers[0])\n",
    "        \n",
    "        self.W2 = np.random.rand(layers[1],layers[0])\n",
    "        self.b2 = np.random.rand(1,layers[1])\n",
    "        \n",
    "        self.W3 = np.random.rand(layers[2],layers[1])\n",
    "        self.b3 = np.random.rand(1,layers[2])\n",
    "        \n",
    "        self.model = {'epoch':[],'W1':[],'b1':[],'W2':[],'b2':[],'W3':[],'b3':[]}\n",
    "    \n",
    "    def update_model(self,epoch):\n",
    "        \n",
    "        self.model['epoch'].append(epoch)\n",
    "        self.model['W1'].append(self.W1)\n",
    "        self.model['b1'].append(self.b1)\n",
    "        self.model['W2'].append(self.W2)\n",
    "        self.model['b2'].append(self.b2)\n",
    "        self.model['W3'].append(self.W3)\n",
    "        self.model['b3'].append(self.b3)\n",
    "        \n",
    "    def tanh_derivative(self,a):\n",
    "        \n",
    "        return 1-np.power(a,2)\n",
    "    \n",
    "    def train(self,X,y):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # forward propagation\n",
    "        self.Z1 = np.dot(self.X,self.W1.T)+self.b1\n",
    "        self.A1 = np.tanh(self.Z1)\n",
    "        \n",
    "        assert(self.Z1.shape == self.A1.shape),'Z1 and A1 shapes not equal'\n",
    "        \n",
    "        self.Z2 = np.dot(self.A1,self.W2.T)+self.b2\n",
    "        self.A2 =  np.tanh(self.Z2)\n",
    "        \n",
    "        assert(self.Z2.shape == self.A2.shape),'Z2 and A2 shapes not equal'\n",
    "        \n",
    "        self.Z3 = np.dot(self.A2,self.W3.T)+self.b3\n",
    "        self.y_hat = softmax(self.Z3,axis=1)\n",
    "        \n",
    "        \n",
    "        # backward propagation\n",
    "        self.y = self.y.reshape(self.y_hat.shape) # reshaping y into y_hat's shape to avoid wrong calculations \n",
    "        \n",
    "        #layer 3\n",
    "        self.dZ3 = self.y_hat - self.y \n",
    "        \n",
    "        self.dW3 = (1/self.m) * np.dot(self.dZ3.T,self.A2)\n",
    "        \n",
    "        self.db3 = (1/self.m) * np.sum(self.dZ3,axis=0,keepdims=True)\n",
    "        \n",
    "        assert self.dW3.shape == self.W3.shape\n",
    "        assert self.db3.shape == self.b3.shape\n",
    "\n",
    "        #layer 2\n",
    "        self.dZ2 = np.multiply( np.dot(self.dZ3,self.W3) , self.tanh_derivative(self.A2) )\n",
    "        \n",
    "        assert self.dZ2.shape == self.A2.shape,\"dZ2 and A2 shapes not equal\"\n",
    "        \n",
    "        self.dW2 = (1/self.m) * np.dot(self.dZ2.T,self.A1)\n",
    "        \n",
    "        self.db2 = (1/self.m) * np.sum(self.dZ2,axis=0,keepdims=True)\n",
    "        \n",
    "        assert self.dW2.shape == self.W2.shape\n",
    "        assert self.db2.shape == self.b2.shape\n",
    "        \n",
    "        #layer 1\n",
    "        \n",
    "        self.dZ1 = np.multiply( np.dot(self.dZ2,self.W2) , self.tanh_derivative(self.A1) )\n",
    "        \n",
    "        self.dW1 = (1/self.m) * np.dot(self.dZ1.T,self.X)\n",
    "        \n",
    "        self.db1 = (1/self.m) * np.sum(self.dZ1,axis=0,keepdims=True)\n",
    "        \n",
    "        self.dW3 = self.dW3 + self.reg_lambda * self.dW3   # regularizing to reduce overfitting \n",
    "        self.db3 = self.db3 + self.reg_lambda * self.db3\n",
    "        self.dW2 = self.dW2 + self.reg_lambda * self.dW2   # regularizing to reduce overfitting \n",
    "        self.db2 = self.db2 + self.reg_lambda * self.db2\n",
    "        self.dW1 = self.dW1 + self.reg_lambda * self.dW1        \n",
    "        self.db2 = self.db2 + self.reg_lambda * self.db2\n",
    "        \n",
    "        self.W3 = self.W3 - self.l_rate * self.dW3 # updating weights\n",
    "        self.b3 = self.b3 - self.l_rate * self.db3 # updating bias\n",
    "        self.W2 = self.W2 - self.l_rate * self.dW2 # updating weights\n",
    "        self.b2 = self.b2 - self.l_rate * self.db2 # updating bias\n",
    "        self.W1 = self.W1 - self.l_rate * self.dW1 # updating weights\n",
    "        self.b1 = self.b1 - self.l_rate * self.db1 # updating bias\n",
    "        \n",
    "    def cost(self):\n",
    "\n",
    "        return (-1/self.m) * np.sum ((np.multiply(np.log(self.y_hat),self.y)))\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \n",
    "        Z1 = np.dot(x,self.W1.T) + self.b1 \n",
    "        \n",
    "        A1 = np.tanh(Z1)\n",
    "        \n",
    "        Z2 = np.dot(A1,self.W2.T) + self.b2\n",
    "        \n",
    "        A2 = np.tanh(Z2)\n",
    "        \n",
    "        Z3 = np.dot(A2,self.W3.T) + self.b3\n",
    "        \n",
    "        y_hat = softmax(Z3)  \n",
    "\n",
    "        preds = np.argmax(y_hat,axis=1)\n",
    "\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/Srujan/Documents/Datasets/fashion-mnist_train.csv')\n",
    "X = train.iloc[:,1:]\n",
    "y = train.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(sparse=False)\n",
    "y_transformed_train = onehot.fit_transform(y_train)\n",
    "y_transformed_test = onehot.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_transformed_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.02,0.01]\n",
    "n_classes = y_transformed_train.shape[1]\n",
    "n_samples = X_train.shape[1]\n",
    "metrics = {'learning_rate':[],'nodes':[],'reg_param':[],'batch_size':[],'train_accuracy':[],'test_accuracy':[]}\n",
    "\n",
    "for lr in learning_rates: # tuning learning rate\n",
    "    \n",
    "    # tuning nodes in hidden layers\n",
    "    for nodes in [[2,3,n_classes]]:\n",
    "        \n",
    "        for reg_param in [0.003,0.005]: # tuning regularization parameter\n",
    "            \n",
    "            for batch_size in [128]: # tuning batch size\n",
    "                cost_list = []\n",
    "                n = NeuralNetwork(n_samples,nodes,lr,reg_param,batch_size)\n",
    "                \n",
    "                for epoch in range(0,2000):\n",
    "                    start = 0                                                #batch starting index\n",
    "                    end = batch_size\n",
    "                    while (end <= X_train.shape[0]):\n",
    "\n",
    "                        x_trainable = X_train[start:end,:]                    \n",
    "                        y_trainable = y_transformed_train[start:end,:]\n",
    "                        n.train(x_trainable,y_trainable)                     # batch size training\n",
    "                        start = start+batch_size\n",
    "                        end = end+batch_size                                 # updating indexes of batch size \n",
    "                   \n",
    "                    cost_list.append(n.cost())\n",
    "                    n.update_model(epoch)   \n",
    "                    \n",
    "                plt.figure(figsize=(12,4)) # plotting cost for every learning rate\n",
    "                x_vals = list(range(500))\n",
    "                sns.lineplot(x=x_vals,y=cost_list)\n",
    "                plt.title(f'plottig cost with learning rate {lr}')  \n",
    "                \n",
    "                y_train_preds = n.predict(X_train)  # converting output of sigmoid into predictions\n",
    "                y_test_preds = n.predict(X_test)\n",
    "\n",
    "                train_accuracy = accuracy_score(y_train,y_train_preds)\n",
    "                test_accuracy = accuracy_score(y_test,y_test_preds)\n",
    "                \n",
    "                # adding to dictionary\n",
    "                metrics['learning_rate'].append(lr)\n",
    "                metrics['nodes'].append(nodes)\n",
    "                metrics['reg_param'].append(reg_param)\n",
    "                metrics['batch_size'].append(batch_size)\n",
    "                metrics['train_accuracy'].append(train_accuracy)\n",
    "                metrics['test_accuracy'].append(test_accuracy)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(metrics)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('learning_rate', [0.01]), ('train_accuracy', [0.1]), ('test_accuracy', [0.1])])\n",
      "dict_items([('learning_rate', [0.01, 0.003]), ('train_accuracy', [0.1, 0.1]), ('test_accuracy', [0.1, 0.1])])\n",
      "dict_items([('learning_rate', [0.01, 0.003, 0.005]), ('train_accuracy', [0.1, 0.1, 0.1]), ('test_accuracy', [0.1, 0.1, 0.1])])\n",
      "dict_items([('learning_rate', [0.01, 0.003, 0.005, 0.001]), ('train_accuracy', [0.1, 0.1, 0.1, 0.1]), ('test_accuracy', [0.1, 0.1, 0.1, 0.1])])\n",
      "dict_items([('learning_rate', [0.01, 0.003, 0.005, 0.001, 0.0005]), ('train_accuracy', [0.1, 0.1, 0.1, 0.1, 0.1]), ('test_accuracy', [0.1, 0.1, 0.1, 0.1, 0.1])])\n",
      "Wall time: 22min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learning_rates = [0.01,0.003,0.005,0.001,0.0005]\n",
    "n_classes = y_transformed_train.shape[1]\n",
    "n_samples = X_train.shape[1]\n",
    "\n",
    "batch_size=64\n",
    "metrics = {'learning_rate':[],'train_accuracy':[],'test_accuracy':[]}\n",
    "\n",
    "for lr in learning_rates: \n",
    "    \n",
    "    n = NeuralNetwork(n_samples,[3,3,n_classes],lr,0.003,batch_size)\n",
    "\n",
    "    for epoch in range(0,500):\n",
    "        start = 0                                                #batch starting index\n",
    "        end = batch_size\n",
    "        while (end <= X_train.shape[0]):\n",
    "\n",
    "            x_trainable = X_train[start:end,:]                    \n",
    "            y_trainable = y_transformed_train[start:end,:]\n",
    "            n.train(x_trainable,y_trainable)                     # batch size training\n",
    "            start = start+batch_size\n",
    "            end = end+batch_size                                 # updating indexes of batch size \n",
    "\n",
    "        n.update_model(epoch)   \n",
    "\n",
    "    y_train_preds = n.predict(X_train)  # converting output of sigmoid into predictions\n",
    "    y_test_preds = n.predict(X_test)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train,y_train_preds)\n",
    "    test_accuracy = accuracy_score(y_test,y_test_preds)\n",
    "    #adding to dictionary\n",
    "    metrics['learning_rate'].append(lr)\n",
    "    metrics['train_accuracy'].append(train_accuracy)\n",
    "    metrics['test_accuracy'].append(test_accuracy)   \n",
    "    print(metrics.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAD4CAYAAAAejHvMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcEUlEQVR4nO3dcYxd5Xnn8e/v3pnBMCYJ1NMmNaZOocLJZkmg3iyt2arbtN1UrWIQuyJZBbqlEtUqbaFC2035J1rxz1qqULurbhEb0ma33kQUsJYNbRNEiVikjRvbcXHMkJaSpDFxwpQ0YKcGPJ5n/7hnzJ3bseeOuTPXnvv9SJbvnPOeO8/1a5sfr5/znlQVkiRJkl7XGnYBkiRJ0tnGkCxJkiT1MCRLkiRJPQzJkiRJUg9DsiRJktRjbNgFLGbDhg21efPmYZchSZKkNWzv3r1/V1VTi507K0Py5s2b2bNnz7DLkCRJ0hqW5OunOrdku0WSTUkeTzKd5GCS2xYZsz3JU0n2J9mT5Nquc7/RXPflJJ9Ksu7MP4okSZK08vrpSZ4F7qiqdwDXAB9J8s6eMY8B766q9wC3AB8HSLIR+HVga1W9C2gDHxxU8ZIkSdJKWDIkV9XhqtrXvD4CTAMbe8Ycrdcf3TcJdD/Gbww4P8kYcAHwzUEULkmSJK2UZe1ukWQzcBWwe5Fz1yd5BniEzmoyVfU88NvA3wKHgZeq6nOneO9bm1aNPTMzM8spS5IkSRqovkNykvXAg8DtVfVy7/mq2lVVW4DrgLuaay4CtgNvB34QmEzy4cXev6ruraqtVbV1amrRmwwlSZKkVdFXSE4yTicg76yqh043tqqeAC5LsgH4aeCrVTVTVceBh4Aff4M1S5IkSSuqn90tAtwHTFfV3acYc3kzjiRXAxPAi3TaLK5JckFz/n10epolSZKks1Y/+yRvA24CDiTZ3xy7E7gUoKruAW4Abk5yHDgG3NjcyLc7yQPAPjq7ZHwJuHewH2Ew/njPN5ir4sZ/dumwS5EkSdKQLRmSq+pJIEuM2QHsOMW5jwEfO6PqVtGuLz3P8RNzhmRJkiQtb3eLtayVMFdLj5MkSdLaZ0huJDBXpmRJkiQZkk9qJcy5lCxJkiQMySe1gu0WkiRJAgzJJ7Vbsd1CkiRJgCH5pHjjniRJkhqG5EYrUK4kS5IkCUPySa2EEy4lS5IkCUPySS17kiVJktQwJDdaCWZkSZIkgSH5pJYPE5EkSVLDkNxoJZwwJEuSJAlD8kkJzM0NuwpJkiSdDQzJjXbiFnCSJEkCDMkntXyYiCRJkhqG5Ear5Y17kiRJ6jAkNzqPpTYkS5IkqY+QnGRTkseTTCc5mOS2RcZsT/JUkv1J9iS5tjl+RXNs/sfLSW5fiQ/yRrVtt5AkSVJjrI8xs8AdVbUvyYXA3iSPVtXTXWMeAx6uqkpyJXA/sKWqvgK8ByBJG3ge2DXYjzAY7pMsSZKkeUuuJFfV4ara17w+AkwDG3vGHK3Xt4aYBBZLm+8D/qaqvv7GSl4ZSZhzKVmSJEkssyc5yWbgKmD3IueuT/IM8AhwyyKXfxD41Gne+9amVWPPzMzMcsoaCHe3kCRJ0ry+Q3KS9cCDwO1V9XLv+araVVVbgOuAu3qunQA+APzxqd6/qu6tqq1VtXVqaqrfsgbGdgtJkiTN6yskJxmnE5B3VtVDpxtbVU8AlyXZ0HX454B9VfXtM650hbVb7m4hSZKkjn52twhwHzBdVXefYszlzTiSXA1MAC92DfkQp2m1OBvEdgtJkiQ1+tndYhtwE3Agyf7m2J3ApQBVdQ9wA3BzkuPAMeDG+Rv5klwA/AzwKwOufaBawcdSS5IkCegjJFfVk0CWGLMD2HGKc/8AfN8ZVbeKWgknXEqWJEkSPnHvpFbLdgtJkiR1GJIbrWat3JYLSZIkGZIbrc59h64mS5IkyZA8b34l2b5kSZIkGZIbObmSbEiWJEkadYbkRrtZSjYjS5IkyZDcmG+3cCVZkiRJhuRGy3YLSZIkNQzJjZM9yXNDLkSSJElDZ0hutG23kCRJUsOQ3Gi1bLeQJElShyG5ER8mIkmSpIYhueHuFpIkSZpnSG64u4UkSZLmGZIbbdstJEmS1DAkNzLfbmFKliRJGnmG5MZ8u4XdFpIkSVoyJCfZlOTxJNNJDia5bZEx25M8lWR/kj1Jru0695YkDyR5pnmPHxv0hxiEVvMrccKULEmSNPLG+hgzC9xRVfuSXAjsTfJoVT3dNeYx4OGqqiRXAvcDW5pzvwv8WVX96yQTwAWD/ACD4o17kiRJmrdkSK6qw8Dh5vWRJNPARuDprjFHuy6ZBAogyZuAnwD+XTPuNeC1AdU+UK+3WxiSJUmSRt2yepKTbAauAnYvcu76JM8AjwC3NId/GJgB/iDJl5J8PMnkKd771qZVY8/MzMxyyhqIlrtbSJIkqdF3SE6yHngQuL2qXu49X1W7qmoLcB1wV3N4DLga+P2qugr4HvDRxd6/qu6tqq1VtXVqamqZH+ON82EikiRJmtdXSE4yTicg76yqh043tqqeAC5LsgE4BByqqvmV5wfohOazzvxjqU+4lCxJkjTy+tndIsB9wHRV3X2KMZc340hyNTABvFhV3wK+keSKZuj76OplPpu0W24BJ0mSpI5+drfYBtwEHEiyvzl2J3ApQFXdA9wA3JzkOHAMuLFevwPu14Cdzc4WzwG/NMD6B8Z2C0mSJM3rZ3eLJ4EsMWYHsOMU5/YDW8+oulXkjXuSJEma5xP3GvOPpbYnWZIkSYbkxus9yYZkSZKkUWdIbthuIUmSpHmG5Ea8cU+SJEkNQ3Lj9ZVkQ7IkSdKoMyQ3TobkuSEXIkmSpKEzJDfaza+EK8mSJEkyJDdiu4UkSZIahuTGfLuFGVmSJEmG5EbLh4lIkiSpYUhuuLuFJEmS5hmSGz5MRJIkSfMMyY1W8yvhY6klSZJkSG64kixJkqR5huTGyRv3XEmWJEkaeYbkxutbwBmSJUmSRp0hueHuFpIkSZpnSG7Mh+QTc0MuRJIkSUO3ZEhOsinJ40mmkxxMctsiY7YneSrJ/iR7klzbde5rSQ7Mnxv0BxiU+d0t5rxzT5IkaeSN9TFmFrijqvYluRDYm+TRqnq6a8xjwMNVVUmuBO4HtnSd/5dV9XeDK3vwxpqUPGtIliRJGnlLriRX1eGq2te8PgJMAxt7xhyt1+94mwTOuaTZbra3cHcLSZIkLasnOclm4Cpg9yLnrk/yDPAIcEvXqQI+l2RvkltP8963Nq0ae2ZmZpZT1kCcDMk2JUuSJI28vkNykvXAg8DtVfVy7/mq2lVVW4DrgLu6Tm2rqquBnwM+kuQnFnv/qrq3qrZW1dapqallfYhBeH0ledW/tSRJks4yfYXkJON0AvLOqnrodGOr6gngsiQbmq+/2fz8ArALeO8bqniFjM2H5DlXkiVJkkZdP7tbBLgPmK6qu08x5vJmHEmuBiaAF5NMNjf7kWQS+Fngy4MqfpDmV5K9cU+SJEn97G6xDbgJOJBkf3PsTuBSgKq6B7gBuDnJceAYcGOz08UPALua/DwG/K+q+rMBf4aBmA/JbgEnSZKkJUNyVT0JZIkxO4Adixx/Dnj3GVe3itpxJVmSJEkdPnGv0WqFBE4YkiVJkkaeIbnLWCuGZEmSJBmSu7ViSJYkSZIheQFXkiVJkgSG5AXarXjjniRJkgzJ3dquJEuSJAlD8gLtVosTZUiWJEkadYbkLu0WnDhhSJYkSRp1huQuY64kS5IkCUPyAvYkS5IkCQzJC7i7hSRJksCQvEC7FeYMyZIkSSPPkNylnTA7NzfsMiRJkjRkhuQu9iRLkiQJDMkLjLUNyZIkSTIkL9CKN+5JkiTJkLzAWCvMuU+yJEnSyFsyJCfZlOTxJNNJDia5bZEx25M8lWR/kj1Jru05307ypSSfGWTxg9ZuhVmfuCdJkjTyxvoYMwvcUVX7klwI7E3yaFU93TXmMeDhqqokVwL3A1u6zt8GTANvGlThK6HdCq/NuruFJEnSqFtyJbmqDlfVvub1ETphd2PPmKNVJ/sUJoGTy7FJLgF+Hvj4oIpeKe1WfCy1JEmSlteTnGQzcBWwe5Fz1yd5BngEuKXr1O8Avwmcdok2ya1Nq8aemZmZ5ZQ1MG4BJ0mSJFhGSE6yHngQuL2qXu49X1W7qmoLcB1wV3PNLwAvVNXepd6/qu6tqq1VtXVqaqrvDzBIY/YkS5IkiT5DcpJxOgF5Z1U9dLqxVfUEcFmSDcA24ANJvgZ8GvipJH/0xkpeOW13t5AkSRL97W4R4D5guqruPsWYy5txJLkamABerKrfqqpLqmoz8EHgz6vqwwOrfsDaLfdJliRJUn+7W2wDbgIOJNnfHLsTuBSgqu4BbgBuTnIcOAbc2HUj3zmj3WoxZ0iWJEkaeUuG5Kp6EsgSY3YAO5YY83ng88uobdWNuZIsSZIkfOLeAq24u4UkSZIMyQuMuQWcJEmSMCQv0LLdQpIkSRiSFxhzCzhJkiRhSF6g3QqzJ077YEBJkiSNAENyFx9LLUmSJDAkLzDWCidst5AkSRp5huQunXYLQ7IkSdKoMyR3GWu3mJ0rzsGHBUqSJGmADMldxludBwu6DZwkSdJoMyR3GR/r/HIcd4cLSZKkkWZI7jLeng/JriRLkiSNMkNyl4l2p93ClWRJkqTRZkjuMta23UKSJEmG5AXm2y3cBk6SJGm0GZK7jDftFq+5kixJkjTSDMldxm23kCRJEn2E5CSbkjyeZDrJwSS3LTJme5KnkuxPsifJtc3xdUn+IslfNtf+p5X4EINyMiTP2m4hSZI0ysb6GDML3FFV+5JcCOxN8mhVPd015jHg4aqqJFcC9wNbgFeBn6qqo0nGgSeT/GlVfWHQH2QQ5tstjs+5kixJkjTKllxJrqrDVbWveX0EmAY29ow5Wq8/y3kSqOZ4VdXR5vh48+OsXaadOLmSbEiWJEkaZcvqSU6yGbgK2L3IueuTPAM8AtzSdbydZD/wAvBoVf2ja5txtzatGntmZmaWU9bAjPkwEUmSJLGMkJxkPfAgcHtVvdx7vqp2VdUW4Drgrq7jJ6rqPcAlwHuTvGux96+qe6tqa1VtnZqaWu7nGAjbLSRJkgR9huSmn/hBYGdVPXS6sVX1BHBZkg09x78LfB54/5mVuvLGbbeQJEkS/e1uEeA+YLqq7j7FmMubcSS5GpgAXkwyleQtzfHzgZ8GnhlU8YM2bruFJEmS6G93i23ATcCBprcY4E7gUoCquge4Abg5yXHgGHBjs9PF24BPJmnTCeT3V9VnBv0hBuVku4X7JEuSJI20JUNyVT0JZIkxO4Adixx/is6NfucEHyYiSZIk8Il7C9huIUmSJDAkL2C7hSRJksCQvMD4mO0WkiRJMiQvMGG7hSRJkjAkLzDWst1CkiRJhuQF2q2QGJIlSZJGnSG5SxLG2y3bLSRJkkacIbnHeCuuJEuSJI04Q3KP8bGWIVmSJGnEGZJ7TLRbvDZrSJYkSRplhuQe68bbvHL8xLDLkCRJ0hAZknusG2/xynFXkiVJkkaZIbnHeWNtXpl1JVmSJGmUGZJ7rBtv8aoryZIkSSPNkNxj3bgryZIkSaPOkNzjvLG2PcmSJEkjzpDc47zxFq+6u4UkSdJIWzIkJ9mU5PEk00kOJrltkTHbkzyVZH+SPUmu7ffas826MbeAkyRJGnVjfYyZBe6oqn1JLgT2Jnm0qp7uGvMY8HBVVZIrgfuBLX1ee1ZZN97iVR8mIkmSNNKWXEmuqsNVta95fQSYBjb2jDlaVdV8OQlUv9eebXyYiCRJkpbVk5xkM3AVsHuRc9cneQZ4BLhlOdeeTdaNt3jFlWRJkqSR1ndITrIeeBC4vape7j1fVbuqagtwHXDXcq5txtza9DPvmZmZWc5nGKjzxtqcmCuOnzAoS5Ikjaq+QnKScTohd2dVPXS6sVX1BHBZkg3Lubaq7q2qrVW1dWpqqu8PMGjrxju/JLZcSJIkja5+drcIcB8wXVV3n2LM5c04klwNTAAv9nPt2WbdeBvAm/ckSZJGWD+7W2wDbgIOJNnfHLsTuBSgqu4BbgBuTnIcOAbc2Ox0ce1i11bVnwzyQwzSurFOSHYlWZIkaXQtGZKr6kkgS4zZAew4k2vPNuedbLdwJVmSJGlU+cS9Hue5kixJkjTyDMk9Js/rhOR/eM2QLEmSNKoMyT0mz+t0oHzv1dkhVyJJkqRhMST3uLAJyUcNyZIkSSPLkNzDlWRJkiQZkntMupIsSZI08gzJPSYnOjfuGZIlSZJGlyG5x1i7xfnjbdstJEmSRpgheRGT541x9FW3gJMkSRpVhuRFrD/PlWRJkqRRZkheRGcl2ZAsSZI0qgzJi1hvSJYkSRpphuRFXLhujKOvGJIlSZJGlSF5EW8+f4KXjh0fdhmSJEkaEkPyIi6eHOc733tt2GVIkiRpSAzJi7hocoJjx09w7DW3gZMkSRpFhuRFXHTBBAB//w+uJkuSJI0iQ/IiDMmSJEmjbcmQnGRTkseTTCc5mOS2RcZsT/JUkv1J9iS5tuvcJ5K8kOTLgy5+pVw82YTk73nzniRJ0ijqZyV5Frijqt4BXAN8JMk7e8Y8Bry7qt4D3AJ8vOvcHwLvH0Ctq+biyXEAvuNKsiRJ0khaMiRX1eGq2te8PgJMAxt7xhytqmq+nASq69wTwHcGVvEqmG+3+M7RV4dciSRJkoZhWT3JSTYDVwG7Fzl3fZJngEforCYvS5Jbm1aNPTMzM8u9fKAuumCCiXaLwy+/MtQ6JEmSNBx9h+Qk64EHgdur6uXe81W1q6q2ANcBdy23kKq6t6q2VtXWqamp5V4+UK1WeOub13H4u4ZkSZKkUdRXSE4yTicg76yqh043tmmvuCzJhgHUNzRve/M6vvWSIVmSJGkU9bO7RYD7gOmquvsUYy5vxpHkamACeHGQha62t715Hd986diwy5AkSdIQjPUxZhtwE3Agyf7m2J3ApQBVdQ9wA3BzkuPAMeDG+Rv5knwK+ElgQ5JDwMeq6r6BfooV8La3nM+3Dxxmbq5otTLsciRJkrSKlgzJVfUkcNqUWFU7gB2nOPehMyttuC69+AKOnyie/+4xNl18wbDLkSRJ0iryiXuncPn3rwfg2ReODrkSSZIkrTZD8ilcPmVIliRJGlWG5FO4aHKC75uc4K++fWTYpUiSJGmVGZJP459e8mb+8tB3h12GJEmSVpkh+TR+9NKL+KtvH+WlY8eHXYokSZJWkSH5NLZuvhiALzx3Tm/5LEmSpGUyJJ/G1s0X8aZ1Y3zu4LeHXYokSZJWkSH5NMbbLX7mnW/lswe/xdFXZ4ddjiRJklaJIXkJN/3YD3H01Vl2fuHrwy5FkiRJq8SQvIT3bHoLP3nFFP/1z5/l6y9+b9jlSJIkaRUYkvtw1/Z3MdYOH7r3Cxw49NKwy5EkSdIKMyT3YdPFF/BHv/zPAfjA7z3Jr/zPPTyw9xBffv4lXjp2nLm5GnKFkiRJGqSxYRdwrnjXxjfzmV//F/z3//sc93/xG3y2Z8eLyYk2F5w3xlgrtBISaCW0mp8TSDKk6jVIzqIkSYN1849v5qZrfmjYZSxgSF6Giycn+I/v38J/+Nkr+OsXjvLczFEO/f0xjrxynKOvnuDY8VlOzBVzBXNVVPPzXOFq8xpROI+SJA3a901ODLuEf8SQfAZarXDFWy/kirdeOOxSJEmStALsSZYkSZJ6GJIlSZKkHkuG5CSbkjyeZDrJwSS3LTJme5KnkuxPsifJtV3n3p/kK0meTfLRQX8ASZIkadD66UmeBe6oqn1JLgT2Jnm0qp7uGvMY8HBVVZIrgfuBLUnawO8BPwMcAr6Y5OGeayVJkqSzypIryVV1uKr2Na+PANPAxp4xR6tq/rb/STi5BcB7gWer6rmqeg34NLB9UMVLkiRJK2FZPclJNgNXAbsXOXd9kmeAR4BbmsMbgW90DTtET8Duuv7WplVjz8zMzHLKkiRJkgaq75CcZD3wIHB7Vb3ce76qdlXVFuA64K75yxZ5q0U3mq2qe6tqa1VtnZqa6rcsSZIkaeD6CslJxukE5J1V9dDpxlbVE8BlSTbQWTne1HX6EuCbZ1irJEmStCryeivxKQZ0nqX8SeA7VXX7KcZcDvxNc+Pe1cD/oROI28BfAe8Dnge+CPzbqjq4xPecAb6+zM8yCBuAvxvC99Xqcp5Hg/M8Gpzntc85Hg3DmucfqqpFWxj62d1iG3ATcCDJ/ubYncClAFV1D3ADcHOS48Ax4MbmRr7ZJL8KfJZOYP7EUgG5ec+h9Fsk2VNVW4fxvbV6nOfR4DyPBud57XOOR8PZOM9LhuSqepLFe4u7x+wAdpzi3J8Af3JG1UmSJElD4BP3JEmSpB6G5IXuHXYBWhXO82hwnkeD87z2Ocej4ayb5yVv3JMkSZJGjSvJkiRJUg9DsiRJktTDkAwkeX+SryR5NslHh12PzlySTUkeTzKd5GCS25rjFyd5NMlfNz9f1HXNbzVz/5Uk/2p41Wu5krSTfCnJZ5qvnec1JslbkjyQ5Jnmz/WPOc9rT5LfaP7O/nKSTyVZ5zyf+5J8IskLSb7cdWzZ85rkR5McaM79l+YZHitu5ENykjbwe8DPAe8EPpTkncOtSm/ALHBHVb0DuAb4SDOfHwUeq6ofAR5rvqY590HgnwDvB/5b83tC54bbgOmur53nted3gT+rqi3Au+nMt/O8hiTZCPw6sLWq3kXnuQofxHleC/6Qzhx1O5N5/X3gVuBHmh+977kiRj4kA+8Fnq2q56rqNeDTwPYh16QzVFWHq2pf8/oInf+gbqQzp59shn0SuK55vR34dFW9WlVfBZ6l83tCZ7kklwA/D3y867DzvIYkeRPwE8B9AFX1WlV9F+d5LRoDzk8yBlwAfBPn+ZxXVU8A3+k5vKx5TfI24E1V9f+aB9X9j65rVpQhuROgvtH19aHmmM5xSTYDVwG7gR+oqsPQCdLA9zfDnP9z1+8AvwnMdR1znteWHwZmgD9o2mo+nmQS53lNqarngd8G/hY4DLxUVZ/DeV6rljuvG5vXvcdXnCF58acJui/eOS7JeuBB4Paqevl0Qxc55vyf5ZL8AvBCVe3t95JFjjnPZ78x4Grg96vqKuB7NP80ewrO8zmo6UndDrwd+EFgMsmHT3fJIsec53PfqeZ1aPNtSO78H8mmrq8vofPPPDpHJRmnE5B3VtVDzeFvN/9kQ/PzC81x5//ctA34QJKv0WmR+qkkf4TzvNYcAg5V1e7m6wfohGbneW35aeCrVTVTVceBh4Afx3leq5Y7r4ea173HV5whGb4I/EiStyeZoNM0/vCQa9IZau54vQ+Yrqq7u049DPxi8/oXgf/ddfyDSc5L8nY6NwT8xWrVqzNTVb9VVZdU1WY6f2b/vKo+jPO8plTVt4BvJLmiOfQ+4Gmc57Xmb4FrklzQ/B3+Pjr3kzjPa9Oy5rVpyTiS5Jrm98fNXdesqLHV+CZns6qaTfKrwGfp3FH7iao6OOSydOa2ATcBB5Lsb47dCfxn4P4kv0znL+R/A1BVB5PcT+c/vLPAR6rqxOqXrQFxnteeXwN2NosYzwG/RGeBx3leI6pqd5IHgH105u1LdB5RvB7n+ZyW5FPATwIbkhwCPsaZ/T397+nslHE+8KfNj5Wv38dSS5IkSQvZbiFJkiT1MCRLkiRJPQzJkiRJUg9DsiRJktTDkCxJkiT1MCRLkiRJPQzJkiRJUo//DwsEB+RTAhFwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_classes = y_transformed_train.shape[1]\n",
    "n_samples = X_train.shape[1]\n",
    "batch_size=128\n",
    "\n",
    "n = NeuralNetwork(n_samples,[3,3,n_classes],0.002,0.003,batch_size)\n",
    "cost_list = []\n",
    "\n",
    "for epoch in range(0,1000):\n",
    "    start = 0                                                #batch starting index\n",
    "    end = batch_size\n",
    "    while (end <= X_train.shape[0]):\n",
    "\n",
    "        x_trainable = X_train[start:end,:]                    \n",
    "        y_trainable = y_transformed_train[start:end,:]\n",
    "        n.train(x_trainable,y_trainable)                     # batch size training\n",
    "        start = start+batch_size\n",
    "        end = end+batch_size                                 # updating indexes of batch size \n",
    "    \n",
    "    cost_list.append(n.cost())\n",
    "    n.update_model(epoch)   \n",
    "    \n",
    "plt.figure(figsize=(12,4)) # plotting cost for every learning rate\n",
    "x_vals = list(range(1000))\n",
    "sns.lineplot(x=x_vals,y=cost_list)\n",
    "\n",
    "y_train_preds = n.predict(X_train)  # converting output of sigmoid into predictions\n",
    "y_test_preds = n.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train,y_train_preds)\n",
    "test_accuracy = accuracy_score(y_test,y_test_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
